{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS155 Miniproject 3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK9jAgbebJQS",
        "colab_type": "text"
      },
      "source": [
        "## Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-MZwa8jpmO1",
        "colab_type": "text"
      },
      "source": [
        "To pre-process the data, we'll tokenize the lines into words and remove punctuation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyZd0Zx6g2W6",
        "colab_type": "code",
        "outputId": "b1450514-02ec-4771-b07f-f5413a2056f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "# Load the data\n",
        "shakespeare = [w.lower() for w in (open('shakespeare.txt', 'r').read()).split('\\n')]\n",
        "nltk.download('punkt')\n",
        "\n",
        "grammar = [\",\", \";\", \".\", \":\", \"?\", \"(\", \")\", \"!\"]\n",
        "word_list = []\n",
        "\n",
        "for i in range(len(shakespeare)):\n",
        "  if len(shakespeare[i]) > len(shakespeare[0]) + 2: # don't read the number lines\n",
        "    line = [w for w in nltk.word_tokenize(shakespeare[i]) if w not in grammar]\n",
        "    word_list.extend(line)\n",
        "\n",
        "word_list = list(set(word_list))\n",
        "num_words = len(word_list)\n",
        "\n",
        "# Create dictionaries of the words to give them index values for easier training\n",
        "word_int = {word:i for i, word in enumerate(word_list)}\n",
        "int_word = {i:word for i, word in enumerate(word_list)}\n",
        "\n",
        "# Create training data\n",
        "train_X = []\n",
        "\n",
        "for i in range(len(shakespeare)):\n",
        "  if len(shakespeare[i]) > len(shakespeare[0]) + 2: # don't read the number lines\n",
        "      line = [int(word_int[w]) for w in nltk.word_tokenize(shakespeare[i]) if w not in grammar]\n",
        "      train_X.append(line)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uruTPvJiEPO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in syllable dictionary\n",
        "dictf = open('Syllable_dictionary.txt', 'r').read().split('\\n')\n",
        "syll_dict = {}\n",
        "end_dict = {}\n",
        "\n",
        "for line in dictf:\n",
        "  arr = line.split(' ')\n",
        "  if len(arr) == 2:\n",
        "    syll_dict[arr[0]] = int(arr[1])\n",
        "  else:\n",
        "    for i in range(1, len(arr)):\n",
        "      if len(arr[i]) > 1:\n",
        "        end_dict[arr[0]] = int(arr[i][1])\n",
        "      else:\n",
        "        syll_dict[arr[0]] = int(arr[i])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KkTUrlIiEr7",
        "colab_type": "text"
      },
      "source": [
        "## Unsupervised Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB-tWdp7o2w3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(val):\n",
        "    '''\n",
        "    Normalize an array.\n",
        "    \n",
        "    Arguments:\n",
        "        val:        Given array to normalize.\n",
        "    '''\n",
        "    val = np.copy(val)\n",
        "    val /= val.sum()\n",
        "    return np.nan_to_num(val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ysqk_2SUiIby",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class HiddenMarkovModel:\n",
        "    '''\n",
        "    Class implementation of Hidden Markov Models.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, A, O):\n",
        "        '''\n",
        "        Initializes an HMM. Assumes the following:\n",
        "            - States and observations are integers starting from 0.\n",
        "            - There is a start state (see notes on A_start below). There\n",
        "              is no integer associated with the start state, only\n",
        "              probabilities in the vector A_start.\n",
        "            - There is no end state.\n",
        "\n",
        "        Arguments:\n",
        "            A:          Transition matrix with dimensions L x L.\n",
        "                        The (i, j)^th element is the probability of\n",
        "                        transitioning from state i to state j. Note that\n",
        "                        this does not include the starting probabilities.\n",
        "\n",
        "            O:          Observation matrix with dimensions L x D.\n",
        "                        The (i, j)^th element is the probability of\n",
        "                        emitting observation j given state i.\n",
        "\n",
        "        Parameters:\n",
        "            L:          Number of states.\n",
        "\n",
        "            D:          Number of observations.\n",
        "\n",
        "            A:          The transition matrix.\n",
        "\n",
        "            O:          The observation matrix.\n",
        "\n",
        "            A_start:    Starting transition probabilities. The i^th element\n",
        "                        is the probability of transitioning from the start\n",
        "                        state to state i. For simplicity, we assume that\n",
        "                        this distribution is uniform.\n",
        "        '''\n",
        "\n",
        "        self.L = len(A)\n",
        "        self.D = len(O[0])\n",
        "        self.A = A\n",
        "        self.O = O\n",
        "        self.A_start = [1. / self.L for _ in range(self.L)]\n",
        "\n",
        "\n",
        "    def forward(self, x, normalize=False):\n",
        "        '''\n",
        "        Uses the forward algorithm to calculate the alpha probability\n",
        "        vectors corresponding to a given input sequence.\n",
        "\n",
        "        Arguments:\n",
        "            x:          Input sequence in the form of a list of length M,\n",
        "                        consisting of integers ranging from 0 to D - 1.\n",
        "\n",
        "            normalize:  Whether to normalize each set of alpha_j(i) vectors\n",
        "                        at each i. This is useful to avoid underflow in\n",
        "                        unsupervised learning.\n",
        "\n",
        "        Returns:\n",
        "            alphas:     Vector of alphas.\n",
        "\n",
        "                        The (i, j)^th element of alphas is alpha_j(i),\n",
        "                        i.e. the probability of observing prefix x^1:i\n",
        "                        and state y^i = j.\n",
        "\n",
        "                        e.g. alphas[1][0] corresponds to the probability\n",
        "                        of observing x^1:1, i.e. the first observation,\n",
        "                        given that y^1 = 0, i.e. the first state is 0.\n",
        "        '''\n",
        "        \n",
        "        M = len(x)      # Length of sequence.\n",
        "        alphas = np.array([np.array([0. for _ in range(self.L)]) for _ in range(M + 1)])\n",
        "        alphas[1, :] = np.max(self.A_start * np.array(self.O)[np.newaxis, :, x[0]].T, 1)\n",
        "\n",
        "        # Forward algorithm\n",
        "        for i in range(2, M + 1):\n",
        "          alphas[i, :] = alphas[i - 1].dot(np.array(self.A)) * np.array(self.O)[:, x[i - 1]]\n",
        "\n",
        "        if normalize:\n",
        "          alphas /= alphas.sum(axis=1, keepdims=True)\n",
        "          alphas = np.nan_to_num(alphas) # in case we divide by 0\n",
        "\n",
        "        return alphas\n",
        "\n",
        "\n",
        "    def backward(self, x, normalize=False):\n",
        "        '''\n",
        "        Uses the backward algorithm to calculate the beta probability\n",
        "        vectors corresponding to a given input sequence.\n",
        "\n",
        "        Arguments:\n",
        "            x:          Input sequence in the form of a list of length M,\n",
        "                        consisting of integers ranging from 0 to D - 1.\n",
        "\n",
        "            normalize:  Whether to normalize each set of alpha_j(i) vectors\n",
        "                        at each i. This is useful to avoid underflow in\n",
        "                        unsupervised learning.\n",
        "\n",
        "        Returns:\n",
        "            betas:      Vector of betas.\n",
        "\n",
        "                        The (i, j)^th element of betas is beta_j(i), i.e.\n",
        "                        the probability of observing prefix x^(i+1):M and\n",
        "                        state y^i = j.\n",
        "\n",
        "                        e.g. betas[M][0] corresponds to the probability\n",
        "                        of observing x^M+1:M, i.e. no observations,\n",
        "                        given that y^M = 0, i.e. the last state is 0.\n",
        "        '''\n",
        "        M = len(x)      # Length of sequence.\n",
        "        betas = np.array([np.array([0. for _ in range(self.L)]) for _ in range(M + 1)])\n",
        "        betas[M] = np.ones((np.array(self.A).shape[0]))\n",
        " \n",
        "        # Backwards algorithm\n",
        "        for i in range(M - 1, -1, -1):\n",
        "            for j in range(self.L):\n",
        "                betas[i, j] = (betas[i + 1] * np.array(self.O)[:, x[i]]).dot(np.array(self.A)[j, :])\n",
        "        \n",
        "        if normalize:\n",
        "            betas /= betas.sum(axis=1, keepdims=True)\n",
        "            betas = np.nan_to_num(betas) # in case we divide by 0\n",
        "\n",
        "        return betas\n",
        "\n",
        "\n",
        "    def unsupervised_learning(self, X, N_iters):\n",
        "        '''\n",
        "        Trains the HMM using the Baum-Welch algorithm on an unlabeled\n",
        "        datset X. Note that this method does not return anything, but\n",
        "        instead updates the attributes of the HMM object.\n",
        "\n",
        "        Arguments:\n",
        "            X:          A dataset consisting of input sequences in the form\n",
        "                        of lists of length M, consisting of integers ranging\n",
        "                        from 0 to D - 1. In other words, a list of lists.\n",
        "\n",
        "            N_iters:    The number of iterations to train on.\n",
        "        '''\n",
        "        self.A = np.array(self.A)\n",
        "        self.O = np.array(self.O)\n",
        "\n",
        "        for n in range(N_iters):\n",
        "          # Initialize\n",
        "          A_num = np.zeros(np.shape(self.A))\n",
        "          O_num = np.zeros(np.shape(self.O))\n",
        "          A_denom = np.zeros(self.L)\n",
        "          O_denom = np.zeros(self.L)\n",
        "              \n",
        "          for x in X:\n",
        "              M = len(x)\n",
        "              \n",
        "              # E step\n",
        "              alphas = self.forward(x, True) # normalize\n",
        "              betas = self.backward(x, True) # normalize\n",
        "\n",
        "              # M step: go through each value\n",
        "              # Calculate gamma for the denominator of A and the numerator/denominator of O\n",
        "              gamma = np.zeros((M+1, self.L))\n",
        "              for t in range (1, M + 1):\n",
        "                  gamma[t, :] = alphas[t, :].T * betas[t, :]\n",
        "              # Normalize\n",
        "              gamma /= gamma.sum(axis=1, keepdims=True)\n",
        "              gamma = np.nan_to_num(gamma) # in case we divide by 0\n",
        "\n",
        "              # Calculate the numerator for A\n",
        "              xi = np.zeros((M, self.L, self.L))\n",
        "              for t in range(1, M):\n",
        "                  for i in range(self.L):\n",
        "                      xi[t,i,:] = alphas[t, i] * self.A[i, :] * self.O[:, x[t]].T * betas[t + 1, :].T\n",
        "\n",
        "              # Normalize\n",
        "              for xi_t in xi[1:]:\n",
        "                  if np.sum(xi_t) > 0:\n",
        "                      xi_t /= np.sum(xi_t)\n",
        "\n",
        "              # Update the numerators and denominators\n",
        "              A_num += np.sum(xi, axis=0)\n",
        "              A_denom += np.sum(gamma[1:M], axis=0)\n",
        "              for t in range(1, M + 1):\n",
        "                  O_denom += gamma[t]\n",
        "                  for i in range(self.L):\n",
        "                      O_num[i][x[t-1]] += gamma[t][i]\n",
        "          \n",
        "          self.A = np.copy(np.nan_to_num(A_num / A_denom[:, np.newaxis]))\n",
        "          self.O = np.copy(np.nan_to_num(O_num / O_denom[:, np.newaxis]))\n",
        "        \n",
        "\n",
        "    def generate_emission(self, M):\n",
        "        '''\n",
        "        Generates an emission of length M, assuming that the starting state\n",
        "        is chosen uniformly at random. \n",
        "\n",
        "        Arguments:\n",
        "            M:          Length of the emission to generate.\n",
        "\n",
        "        Returns:\n",
        "            emission:   The randomly generated emission as a list.\n",
        "\n",
        "            states:     The randomly generated states as a list.\n",
        "        '''\n",
        "        # Initialize and pick a random state to start\n",
        "        states = [np.random.randint(self.L)]\n",
        "        emission = [np.random.choice(range(self.D), p = normalize(self.O[states[0]]))]\n",
        "        \n",
        "        for i in range(M - 1):\n",
        "            # Pick the next state depending on the probability\n",
        "            choice = np.random.choice(range(self.L), p = normalize(self.A_start)) if i == 0 else np.random.choice(range(self.L), p = normalize(self.A[states[i]]))\n",
        "            states.append(choice)\n",
        "            emission.append(np.random.choice(range(self.D), p = normalize(self.O[states[i + 1]])))\n",
        "\n",
        "        return emission, states\n",
        "\n",
        "    def probability_alphas(self, x):\n",
        "        '''\n",
        "        Finds the maximum probability of a given input sequence using\n",
        "        the forward algorithm.\n",
        "\n",
        "        Arguments:\n",
        "            x:          Input sequence in the form of a list of length M,\n",
        "                        consisting of integers ranging from 0 to D - 1.\n",
        "\n",
        "        Returns:\n",
        "            prob:       Total probability that x can occur.\n",
        "        '''\n",
        "\n",
        "        # Calculate alpha vectors.\n",
        "        alphas = self.forward(x)\n",
        "\n",
        "        # alpha_j(M) gives the probability that the state sequence ends\n",
        "        # in j. Summing this value over all possible states j gives the\n",
        "        # total probability of x paired with any state sequence, i.e.\n",
        "        # the probability of x.\n",
        "        prob = sum(alphas[-1])\n",
        "        return prob\n",
        "\n",
        "\n",
        "    def probability_betas(self, x):\n",
        "        '''\n",
        "        Finds the maximum probability of a given input sequence using\n",
        "        the backward algorithm.\n",
        "\n",
        "        Arguments:\n",
        "            x:          Input sequence in the form of a list of length M,\n",
        "                        consisting of integers ranging from 0 to D - 1.\n",
        "\n",
        "        Returns:\n",
        "            prob:       Total probability that x can occur.\n",
        "        '''\n",
        "\n",
        "        betas = self.backward(x)\n",
        "\n",
        "        # beta_j(1) gives the probability that the state sequence starts\n",
        "        # with j. Summing this, multiplied by the starting transition\n",
        "        # probability and the observation probability, over all states\n",
        "        # gives the total probability of x paired with any state\n",
        "        # sequence, i.e. the probability of x.\n",
        "        prob = sum([betas[1][j] * self.A_start[j] * self.O[j][x[0]] \\\n",
        "                    for j in range(self.L)])\n",
        "\n",
        "        return prob\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiUyf2l-iIuu",
        "colab_type": "text"
      },
      "source": [
        "## Poetry Generation, Part 1: Hidden Markov Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yDkTUP1iLDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unsupervised_HMM(X, n_states, N_iters):\n",
        "    '''\n",
        "    Helper function to train an unsupervised HMM. The function determines the\n",
        "    number of unique observations in the given data, initializes\n",
        "    the transition and observation matrices, creates the HMM, and then runs\n",
        "    the training function for unsupervised learing.\n",
        "\n",
        "    Arguments:\n",
        "        X:          A dataset consisting of input sequences in the form\n",
        "                    of lists of variable length, consisting of integers \n",
        "                    ranging from 0 to D - 1. In other words, a list of lists.\n",
        "\n",
        "        n_states:   Number of hidden states to use in training.\n",
        "        \n",
        "        N_iters:    The number of iterations to train on.\n",
        "    '''\n",
        "\n",
        "    # Make a set of observations.\n",
        "    observations = set()\n",
        "    for x in X:\n",
        "        observations |= set(x)\n",
        "\n",
        "    \n",
        "    # Compute L and D.\n",
        "    L = n_states\n",
        "    D = max(observations) + 1\n",
        "\n",
        "    # Randomly initialize and normalize matrix A.\n",
        "    random.seed(2020)\n",
        "    A = [[random.random() for i in range(L)] for j in range(L)]\n",
        "\n",
        "    for i in range(len(A)):\n",
        "        norm = sum(A[i])\n",
        "        for j in range(len(A[i])):\n",
        "            A[i][j] /= norm\n",
        "    \n",
        "    # Randomly initialize and normalize matrix O.\n",
        "    random.seed(155)\n",
        "    O = [[random.random() for i in range(D)] for j in range(L)]\n",
        "\n",
        "    for i in range(len(O)):\n",
        "        norm = sum(O[i])\n",
        "        for j in range(len(O[i])):\n",
        "            O[i][j] /= norm\n",
        "\n",
        "    # Train an HMM with unlabeled data.\n",
        "    HMM = HiddenMarkovModel(A, O)\n",
        "    HMM.unsupervised_learning(X, N_iters)\n",
        "\n",
        "    return HMM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUP4WxFTojWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sequence_generator(hmm, k, M, syllable_dict=syll_dict, int_dict=int_word):\n",
        "    '''\n",
        "    Generates k emissions of length M using the HMM for a given n\n",
        "    and prints the results.\n",
        "\n",
        "    Arguments:\n",
        "        hmm:        HMM to use.\n",
        "        K:          Number of sequences to generate.\n",
        "        M:          Length of emission to generate.\n",
        "        syllable_dict:  Syllable dictionary to use.\n",
        "        int_dict:       Integer to word dictionary to use.\n",
        "    '''\n",
        "    # Generate k input sequences.\n",
        "    for i in range(k):\n",
        "      syllables = 0\n",
        "      while syllables != 10:\n",
        "        # Generate a single input sequence of length m.\n",
        "        emission, states = hmm.generate_emission(M)\n",
        "        syllables = 0\n",
        "        generated = \"\"\n",
        "        for i in emission:\n",
        "          if syllables != 10:\n",
        "            if str(int_dict[i]) not in syllable_dict:\n",
        "              syllables = 100\n",
        "              break\n",
        "            syllables += syllable_dict[str(int_dict[i])]\n",
        "            generated += u\" \" + str(int_dict[i])\n",
        "      \n",
        "      # Print the results.\n",
        "      print(generated.strip().capitalize())\n",
        "\n",
        "    print('')\n",
        "    print('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHwAC4lTpyTl",
        "colab_type": "code",
        "outputId": "c7767c95-42cd-4131-c170-ef537ead5973",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Unsupervised HMM learninig\n",
        "n_states = 10\n",
        "n_iters = 100\n",
        "unsup_HMM = unsupervised_HMM(train_X, n_states, n_iters)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:166: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKvNVJeuqEb_",
        "colab_type": "code",
        "outputId": "54c4a301-c065-4f92-d3db-a351f81c8361",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "# Generate sonnet\n",
        "sequence_generator(unsup_HMM, 14, 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gone things sometime seek the forth to and dead\n",
            "Gravity thy thy hap on to use ill\n",
            "Why behold vanishing dost travelled with\n",
            "Me let out i and thy love in use life\n",
            "With checked subjects in vilest writ for dressed\n",
            "Flatter my this from than thoughts their suffer\n",
            "Disperse how fairer plain the and replete\n",
            "To how thy these a to we essays i\n",
            "Me souls the some as frowns and off purposed\n",
            "With all did live seemly chronicle of\n",
            "Thousand delights i outstripped live be worth\n",
            "This thou is for this nothing waking a\n",
            "That but made the love make i thee with can\n",
            "Or thy sweet not particulars need both\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rqeSnN_iLNi",
        "colab_type": "text"
      },
      "source": [
        "## Poetry Generation, Part 2: Recurrent Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUaqc7c_iO8L",
        "colab_type": "code",
        "outputId": "c90bd1cf-e5a2-468c-f17d-22d2bf9917b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import LSTM, Dense, Dropout, Lambda\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "from keras.utils import plot_model, to_categorical\n",
        "\n",
        "import nltk\n",
        "import sys\n",
        "import numpy as np\n",
        "import random\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pzafY-wrTE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def RepresentsInt(s):\n",
        "    try: \n",
        "        int(s)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False\n",
        "    \n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "shakespeare = load_doc('shakespeare.txt')\n",
        "tokens = shakespeare.split('\\n')\n",
        "for i in range(len(tokens) - 1, -1, -1):\n",
        "    if RepresentsInt(tokens[i]):\n",
        "        tokens.pop(i)\n",
        "raw_text = '\\n'.join(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6KvqS3JrV-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Each sequence has 40 characters. \n",
        "training_length = 40\n",
        "\n",
        "def gen_seqs(text, n):\n",
        "    raw_seqs = list()\n",
        "    for i in range(training_length, len(text), n):\n",
        "\n",
        "        seq = text[i - training_length:i + 1]\n",
        "        # Append every n'th sequence of length 41\n",
        "        raw_seqs.append(seq)\n",
        "    return raw_seqs\n",
        "\n",
        "# Take every 4'th character\n",
        "raw_seqs = gen_seqs(raw_text, 4)\n",
        "\n",
        "# Save tokens to file, one dialog per line\n",
        "def save_doc(lines, filename):\n",
        "    # We use a character that does not appear in the data\n",
        "    data = '*'.join(lines)\n",
        "    file = open(filename, 'w')\n",
        "    file.write(data)\n",
        "    file.close()    \n",
        "# save sequences to file\n",
        "out_filename = 'char_sequences.txt'\n",
        "save_doc(raw_seqs, out_filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WXLglaQrWDu",
        "colab_type": "code",
        "outputId": "0692c035-454f-4050-b060-f30a65d48933",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "in_filename = 'char_sequences.txt'\n",
        "raw_split = load_doc(in_filename)\n",
        "lines = raw_split.split('*')\n",
        "\n",
        "chars = sorted(list(set(raw_text)))\n",
        "mapping = dict((c, i) for i, c in enumerate(chars))\n",
        "r_mapping = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "# Process each sequence of characters\n",
        "sequences = list()\n",
        "for line in lines:\n",
        "    #integer encode line\n",
        "    encoded_seq = [mapping[char] for char in line]\n",
        "    #store\n",
        "    sequences.append(encoded_seq)\n",
        "\n",
        "#vocabulary size\n",
        "vocab_size = len(mapping)\n",
        "print('Vocabulary Size: %d' % vocab_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Size: 61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUWrQV0wrWLW",
        "colab_type": "code",
        "outputId": "7dd7a63e-c86b-44d5-a240-af75361f3799",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sequences = np.array(sequences)\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
        "X = np.array(sequences)\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "print(np.shape(X))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(23639, 40, 61)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_5dOhyhrfpy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build the model.\n",
        "def build_model(temp):\n",
        "    model = Sequential()\n",
        "\n",
        "    # A single recurrent layer, so return sequences is\n",
        "    # set to false\n",
        "    model.add(LSTM(150, return_sequences=False, \n",
        "                   input_shape=(X.shape[1], X.shape[2])))\n",
        "    \n",
        "    model.add(Lambda(lambda x: x / temp))\n",
        "\n",
        "    # Fully connected output layer with softmax activation\n",
        "    model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXjYGx6O5ueN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_nodes = [135, 145, 165, 185, 195]\n",
        "best_nodes = test_nodes[0]\n",
        "min_loss = 10\n",
        "for num_nodes in test_nodes:\n",
        "    test_model = build_model(num_nodes, 1)\n",
        "    callback = [EarlyStopping(monitor='loss', patience=8, \n",
        "                           restore_best_weights = True, mode='min')]\n",
        "    hist = test_model.fit(X, y, batch_size=128, epochs=200, callbacks=callback, verbose=2)\n",
        "    curr_loss = min(hist.history['loss'])\n",
        "    if curr_loss < min_loss:\n",
        "        min_loss = curr_loss\n",
        "        best_nodes = num_nodes\n",
        "print(\"Best number of nodes to use: \" + str(best_nodes))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag1WjqMa5vSG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_bs = [32, 40, 75, 100, 128]\n",
        "best_bs = test_bs[0]\n",
        "min_loss = 10\n",
        "for bs in test_bs:\n",
        "    test_model = build_model(best_nodes, 1)\n",
        "    callback = [EarlyStopping(monitor='loss', patience=8, \n",
        "                           restore_best_weights = True, mode='min')]\n",
        "    hist = test_model.fit(X, y, batch_size=bs, epochs=200, callbacks=callback, verbose=2)\n",
        "    curr_loss = min(hist.history['loss'])\n",
        "    if curr_loss < min_loss:\n",
        "        min_loss = curr_loss\n",
        "        best_bs = bs\n",
        "print(\"Best batch size to use: \" + str(best_bs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URN6tGiQ5yOv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modelt1 = build_model(best_nodes, 0.25)\n",
        "modelt2 = build_model(best_nodes, 0.75)\n",
        "modelt3 = build_model(best_nodes, 1.5)\n",
        "pathname1 = 'models/model_0_25.h5'\n",
        "pathname2 = 'models/model_0_75.h5'\n",
        "pathname3 = 'models/model_1_50.h5'\n",
        "\n",
        "# Create callbacks\n",
        "callback1 = [EarlyStopping(monitor='loss', patience=8, \n",
        "                           restore_best_weights = True, mode='min')]\n",
        "callback2 = [EarlyStopping(monitor='loss', patience=8, \n",
        "                           restore_best_weights = True, mode='min')]\n",
        "callback3 = [EarlyStopping(monitor='loss', patience=8, \n",
        "                           restore_best_weights = True, mode='min')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjKs3mKBrjwm",
        "colab_type": "code",
        "outputId": "6082ddb2-b502-4a96-a2cc-314f792c8b1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "modelt1.fit(X, y, batch_size=best_bs, epochs=200, callbacks=callback1, verbose=2)\n",
        "modelt1.save(pathname1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            " - 14s - loss: 2.9533 - acc: 0.2102\n",
            "Epoch 2/150\n",
            " - 14s - loss: 2.3973 - acc: 0.3248\n",
            "Epoch 3/150\n",
            " - 15s - loss: 2.2384 - acc: 0.3515\n",
            "Epoch 4/150\n",
            " - 15s - loss: 2.1519 - acc: 0.3743\n",
            "Epoch 5/150\n",
            " - 15s - loss: 2.0850 - acc: 0.3921\n",
            "Epoch 6/150\n",
            " - 15s - loss: 2.0218 - acc: 0.4079\n",
            "Epoch 7/150\n",
            " - 15s - loss: 1.9747 - acc: 0.4186\n",
            "Epoch 8/150\n",
            " - 16s - loss: 1.9278 - acc: 0.4284\n",
            "Epoch 9/150\n",
            " - 15s - loss: 1.8854 - acc: 0.4406\n",
            "Epoch 10/150\n",
            " - 15s - loss: 1.8462 - acc: 0.4478\n",
            "Epoch 11/150\n",
            " - 15s - loss: 1.8101 - acc: 0.4580\n",
            "Epoch 12/150\n",
            " - 15s - loss: 1.7702 - acc: 0.4669\n",
            "Epoch 13/150\n",
            " - 15s - loss: 1.7365 - acc: 0.4722\n",
            "Epoch 14/150\n",
            " - 15s - loss: 1.6982 - acc: 0.4832\n",
            "Epoch 15/150\n",
            " - 15s - loss: 1.6637 - acc: 0.4918\n",
            "Epoch 16/150\n",
            " - 15s - loss: 1.6284 - acc: 0.5002\n",
            "Epoch 17/150\n",
            " - 16s - loss: 1.5899 - acc: 0.5103\n",
            "Epoch 18/150\n",
            " - 15s - loss: 1.5493 - acc: 0.5223\n",
            "Epoch 19/150\n",
            " - 15s - loss: 1.5075 - acc: 0.5324\n",
            "Epoch 20/150\n",
            " - 15s - loss: 1.4636 - acc: 0.5480\n",
            "Epoch 21/150\n",
            " - 15s - loss: 1.4223 - acc: 0.5557\n",
            "Epoch 22/150\n",
            " - 15s - loss: 1.3708 - acc: 0.5743\n",
            "Epoch 23/150\n",
            " - 15s - loss: 1.3216 - acc: 0.5869\n",
            "Epoch 24/150\n",
            " - 15s - loss: 1.2683 - acc: 0.6047\n",
            "Epoch 25/150\n",
            " - 15s - loss: 1.2104 - acc: 0.6213\n",
            "Epoch 26/150\n",
            " - 15s - loss: 1.1542 - acc: 0.6399\n",
            "Epoch 27/150\n",
            " - 15s - loss: 1.0920 - acc: 0.6612\n",
            "Epoch 28/150\n",
            " - 15s - loss: 1.0310 - acc: 0.6782\n",
            "Epoch 29/150\n",
            " - 16s - loss: 0.9673 - acc: 0.7001\n",
            "Epoch 30/150\n",
            " - 15s - loss: 0.9107 - acc: 0.7189\n",
            "Epoch 31/150\n",
            " - 15s - loss: 0.8401 - acc: 0.7420\n",
            "Epoch 32/150\n",
            " - 15s - loss: 0.7749 - acc: 0.7634\n",
            "Epoch 33/150\n",
            " - 15s - loss: 0.7134 - acc: 0.7877\n",
            "Epoch 34/150\n",
            " - 15s - loss: 0.6561 - acc: 0.8049\n",
            "Epoch 35/150\n",
            " - 15s - loss: 0.5939 - acc: 0.8282\n",
            "Epoch 36/150\n",
            " - 15s - loss: 0.5375 - acc: 0.8458\n",
            "Epoch 37/150\n",
            " - 15s - loss: 0.4807 - acc: 0.8661\n",
            "Epoch 38/150\n",
            " - 15s - loss: 0.4297 - acc: 0.8845\n",
            "Epoch 39/150\n",
            " - 15s - loss: 0.3828 - acc: 0.9027\n",
            "Epoch 40/150\n",
            " - 15s - loss: 0.3481 - acc: 0.9123\n",
            "Epoch 41/150\n",
            " - 15s - loss: 0.2991 - acc: 0.9306\n",
            "Epoch 42/150\n",
            " - 15s - loss: 0.2651 - acc: 0.9415\n",
            "Epoch 43/150\n",
            " - 15s - loss: 0.2317 - acc: 0.9510\n",
            "Epoch 44/150\n",
            " - 15s - loss: 0.1986 - acc: 0.9620\n",
            "Epoch 45/150\n",
            " - 15s - loss: 0.1799 - acc: 0.9670\n",
            "Epoch 46/150\n",
            " - 15s - loss: 0.1584 - acc: 0.9727\n",
            "Epoch 47/150\n",
            " - 15s - loss: 0.1364 - acc: 0.9799\n",
            "Epoch 48/150\n",
            " - 15s - loss: 0.1126 - acc: 0.9874\n",
            "Epoch 49/150\n",
            " - 16s - loss: 0.1046 - acc: 0.9882\n",
            "Epoch 50/150\n",
            " - 15s - loss: 0.1059 - acc: 0.9854\n",
            "Epoch 51/150\n",
            " - 15s - loss: 0.1126 - acc: 0.9814\n",
            "Epoch 52/150\n",
            " - 15s - loss: 0.1169 - acc: 0.9772\n",
            "Epoch 53/150\n",
            " - 15s - loss: 0.1078 - acc: 0.9804\n",
            "Epoch 54/150\n",
            " - 15s - loss: 0.0702 - acc: 0.9919\n",
            "Epoch 55/150\n",
            " - 15s - loss: 0.0373 - acc: 0.9992\n",
            "Epoch 56/150\n",
            " - 15s - loss: 0.0203 - acc: 1.0000\n",
            "Epoch 57/150\n",
            " - 15s - loss: 0.0146 - acc: 1.0000\n",
            "Epoch 58/150\n",
            " - 16s - loss: 0.0118 - acc: 1.0000\n",
            "Epoch 59/150\n",
            " - 15s - loss: 0.0102 - acc: 1.0000\n",
            "Epoch 60/150\n",
            " - 15s - loss: 0.0090 - acc: 1.0000\n",
            "Epoch 61/150\n",
            " - 15s - loss: 0.0080 - acc: 1.0000\n",
            "Epoch 62/150\n",
            " - 15s - loss: 0.0072 - acc: 1.0000\n",
            "Epoch 63/150\n",
            " - 15s - loss: 0.0065 - acc: 1.0000\n",
            "Epoch 64/150\n",
            " - 15s - loss: 0.0059 - acc: 1.0000\n",
            "Epoch 65/150\n",
            " - 15s - loss: 0.0053 - acc: 1.0000\n",
            "Epoch 66/150\n",
            " - 15s - loss: 0.0048 - acc: 1.0000\n",
            "Epoch 67/150\n",
            " - 15s - loss: 0.0044 - acc: 1.0000\n",
            "Epoch 68/150\n",
            " - 15s - loss: 0.0329 - acc: 0.9921\n",
            "Epoch 69/150\n",
            " - 15s - loss: 1.0297 - acc: 0.6963\n",
            "Epoch 70/150\n",
            " - 16s - loss: 0.3417 - acc: 0.8872\n",
            "Epoch 71/150\n",
            " - 15s - loss: 0.1533 - acc: 0.9587\n",
            "Epoch 72/150\n",
            " - 15s - loss: 0.0779 - acc: 0.9871\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDmcVsocdkkb",
        "colab_type": "code",
        "outputId": "8f3d9b7d-bb27-494b-a032-5d01bbe72ba2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "modelt2.fit(X, y, batch_size=best_bs, epochs=200, callbacks=callback2, verbose=2)\n",
        "modelt2.save(pathname2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            " - 16s - loss: 3.1175 - acc: 0.1709\n",
            "Epoch 2/150\n",
            " - 15s - loss: 2.6865 - acc: 0.2638\n",
            "Epoch 3/150\n",
            " - 15s - loss: 2.3760 - acc: 0.3233\n",
            "Epoch 4/150\n",
            " - 15s - loss: 2.2547 - acc: 0.3509\n",
            "Epoch 5/150\n",
            " - 16s - loss: 2.1605 - acc: 0.3753\n",
            "Epoch 6/150\n",
            " - 16s - loss: 2.0934 - acc: 0.3945\n",
            "Epoch 7/150\n",
            " - 15s - loss: 2.0430 - acc: 0.4051\n",
            "Epoch 8/150\n",
            " - 15s - loss: 1.9974 - acc: 0.4170\n",
            "Epoch 9/150\n",
            " - 15s - loss: 1.9552 - acc: 0.4250\n",
            "Epoch 10/150\n",
            " - 15s - loss: 1.9213 - acc: 0.4354\n",
            "Epoch 11/150\n",
            " - 15s - loss: 1.8845 - acc: 0.4439\n",
            "Epoch 12/150\n",
            " - 16s - loss: 1.8511 - acc: 0.4533\n",
            "Epoch 13/150\n",
            " - 15s - loss: 1.8173 - acc: 0.4586\n",
            "Epoch 14/150\n",
            " - 15s - loss: 1.7877 - acc: 0.4692\n",
            "Epoch 15/150\n",
            " - 15s - loss: 1.7577 - acc: 0.4744\n",
            "Epoch 16/150\n",
            " - 16s - loss: 1.7285 - acc: 0.4806\n",
            "Epoch 17/150\n",
            " - 15s - loss: 1.6976 - acc: 0.4888\n",
            "Epoch 18/150\n",
            " - 17s - loss: 1.6723 - acc: 0.4974\n",
            "Epoch 19/150\n",
            " - 15s - loss: 1.6407 - acc: 0.5056\n",
            "Epoch 20/150\n",
            " - 15s - loss: 1.6108 - acc: 0.5122\n",
            "Epoch 21/150\n",
            " - 15s - loss: 1.5782 - acc: 0.5218\n",
            "Epoch 22/150\n",
            " - 15s - loss: 1.5477 - acc: 0.5287\n",
            "Epoch 23/150\n",
            " - 15s - loss: 1.5183 - acc: 0.5398\n",
            "Epoch 24/150\n",
            " - 15s - loss: 1.4828 - acc: 0.5503\n",
            "Epoch 25/150\n",
            " - 16s - loss: 1.4453 - acc: 0.5612\n",
            "Epoch 26/150\n",
            " - 15s - loss: 1.4071 - acc: 0.5692\n",
            "Epoch 27/150\n",
            " - 15s - loss: 1.3693 - acc: 0.5836\n",
            "Epoch 28/150\n",
            " - 15s - loss: 1.3274 - acc: 0.5967\n",
            "Epoch 29/150\n",
            " - 16s - loss: 1.2859 - acc: 0.6083\n",
            "Epoch 30/150\n",
            " - 15s - loss: 1.2472 - acc: 0.6219\n",
            "Epoch 31/150\n",
            " - 15s - loss: 1.1973 - acc: 0.6373\n",
            "Epoch 32/150\n",
            " - 15s - loss: 1.1544 - acc: 0.6524\n",
            "Epoch 33/150\n",
            " - 15s - loss: 1.1080 - acc: 0.6681\n",
            "Epoch 34/150\n",
            " - 15s - loss: 1.0634 - acc: 0.6825\n",
            "Epoch 35/150\n",
            " - 15s - loss: 1.0133 - acc: 0.6977\n",
            "Epoch 36/150\n",
            " - 15s - loss: 0.9671 - acc: 0.7137\n",
            "Epoch 37/150\n",
            " - 15s - loss: 0.9184 - acc: 0.7303\n",
            "Epoch 38/150\n",
            " - 17s - loss: 0.8730 - acc: 0.7456\n",
            "Epoch 39/150\n",
            " - 15s - loss: 0.8302 - acc: 0.7560\n",
            "Epoch 40/150\n",
            " - 15s - loss: 0.7821 - acc: 0.7748\n",
            "Epoch 41/150\n",
            " - 15s - loss: 0.7450 - acc: 0.7874\n",
            "Epoch 42/150\n",
            " - 15s - loss: 0.6929 - acc: 0.8057\n",
            "Epoch 43/150\n",
            " - 15s - loss: 0.6509 - acc: 0.8206\n",
            "Epoch 44/150\n",
            " - 15s - loss: 0.6153 - acc: 0.8345\n",
            "Epoch 45/150\n",
            " - 16s - loss: 0.5734 - acc: 0.8478\n",
            "Epoch 46/150\n",
            " - 15s - loss: 0.5347 - acc: 0.8608\n",
            "Epoch 47/150\n",
            " - 15s - loss: 0.5043 - acc: 0.8715\n",
            "Epoch 48/150\n",
            " - 15s - loss: 0.4671 - acc: 0.8843\n",
            "Epoch 49/150\n",
            " - 15s - loss: 0.4410 - acc: 0.8912\n",
            "Epoch 50/150\n",
            " - 15s - loss: 0.4017 - acc: 0.9071\n",
            "Epoch 51/150\n",
            " - 15s - loss: 0.3752 - acc: 0.9148\n",
            "Epoch 52/150\n",
            " - 15s - loss: 0.3551 - acc: 0.9213\n",
            "Epoch 53/150\n",
            " - 15s - loss: 0.3345 - acc: 0.9258\n",
            "Epoch 54/150\n",
            " - 15s - loss: 0.3074 - acc: 0.9369\n",
            "Epoch 55/150\n",
            " - 15s - loss: 0.2797 - acc: 0.9444\n",
            "Epoch 56/150\n",
            " - 15s - loss: 0.2728 - acc: 0.9478\n",
            "Epoch 57/150\n",
            " - 15s - loss: 0.2488 - acc: 0.9540\n",
            "Epoch 58/150\n",
            " - 16s - loss: 0.2377 - acc: 0.9571\n",
            "Epoch 59/150\n",
            " - 16s - loss: 0.2365 - acc: 0.9555\n",
            "Epoch 60/150\n",
            " - 16s - loss: 0.2253 - acc: 0.9566\n",
            "Epoch 61/150\n",
            " - 16s - loss: 0.1971 - acc: 0.9664\n",
            "Epoch 62/150\n",
            " - 16s - loss: 0.1839 - acc: 0.9700\n",
            "Epoch 63/150\n",
            " - 16s - loss: 0.1565 - acc: 0.9785\n",
            "Epoch 64/150\n",
            " - 16s - loss: 0.1584 - acc: 0.9750\n",
            "Epoch 65/150\n",
            " - 17s - loss: 0.1719 - acc: 0.9711\n",
            "Epoch 66/150\n",
            " - 16s - loss: 0.1952 - acc: 0.9587\n",
            "Epoch 67/150\n",
            " - 16s - loss: 0.2111 - acc: 0.9522\n",
            "Epoch 68/150\n",
            " - 16s - loss: 0.1719 - acc: 0.9655\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7ALkKFWdniB",
        "colab_type": "code",
        "outputId": "9b85b7f5-188a-47fd-df9d-0eccaff575d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "modelt3.fit(X, y, batch_size=best_bs, epochs=250, callbacks=callback3, verbose=2)\n",
        "modelt3.save(pathname3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            " - 17s - loss: 3.1469 - acc: 0.1684\n",
            "Epoch 2/250\n",
            " - 16s - loss: 2.9813 - acc: 0.1775\n",
            "Epoch 3/250\n",
            " - 16s - loss: 2.7445 - acc: 0.2613\n",
            "Epoch 4/250\n",
            " - 16s - loss: 2.5429 - acc: 0.2994\n",
            "Epoch 5/250\n",
            " - 16s - loss: 2.4252 - acc: 0.3182\n",
            "Epoch 6/250\n",
            " - 16s - loss: 2.3484 - acc: 0.3350\n",
            "Epoch 7/250\n",
            " - 16s - loss: 2.2939 - acc: 0.3449\n",
            "Epoch 8/250\n",
            " - 16s - loss: 2.2409 - acc: 0.3567\n",
            "Epoch 9/250\n",
            " - 17s - loss: 2.1899 - acc: 0.3706\n",
            "Epoch 10/250\n",
            " - 16s - loss: 2.1510 - acc: 0.3817\n",
            "Epoch 11/250\n",
            " - 16s - loss: 2.1171 - acc: 0.3894\n",
            "Epoch 12/250\n",
            " - 16s - loss: 2.0866 - acc: 0.3976\n",
            "Epoch 13/250\n",
            " - 16s - loss: 2.0643 - acc: 0.4027\n",
            "Epoch 14/250\n",
            " - 16s - loss: 2.0445 - acc: 0.4066\n",
            "Epoch 15/250\n",
            " - 17s - loss: 2.0207 - acc: 0.4115\n",
            "Epoch 16/250\n",
            " - 16s - loss: 1.9997 - acc: 0.4177\n",
            "Epoch 17/250\n",
            " - 16s - loss: 1.9819 - acc: 0.4218\n",
            "Epoch 18/250\n",
            " - 16s - loss: 1.9671 - acc: 0.4248\n",
            "Epoch 19/250\n",
            " - 18s - loss: 1.9532 - acc: 0.4295\n",
            "Epoch 20/250\n",
            " - 16s - loss: 1.9349 - acc: 0.4320\n",
            "Epoch 21/250\n",
            " - 16s - loss: 1.9210 - acc: 0.4350\n",
            "Epoch 22/250\n",
            " - 16s - loss: 1.9058 - acc: 0.4409\n",
            "Epoch 23/250\n",
            " - 16s - loss: 1.8927 - acc: 0.4444\n",
            "Epoch 24/250\n",
            " - 16s - loss: 1.8819 - acc: 0.4449\n",
            "Epoch 25/250\n",
            " - 16s - loss: 1.8711 - acc: 0.4477\n",
            "Epoch 26/250\n",
            " - 16s - loss: 1.8572 - acc: 0.4525\n",
            "Epoch 27/250\n",
            " - 16s - loss: 1.8460 - acc: 0.4546\n",
            "Epoch 28/250\n",
            " - 17s - loss: 1.8344 - acc: 0.4573\n",
            "Epoch 29/250\n",
            " - 16s - loss: 1.8241 - acc: 0.4587\n",
            "Epoch 30/250\n",
            " - 16s - loss: 1.8130 - acc: 0.4641\n",
            "Epoch 31/250\n",
            " - 16s - loss: 1.8037 - acc: 0.4644\n",
            "Epoch 32/250\n",
            " - 16s - loss: 1.7911 - acc: 0.4671\n",
            "Epoch 33/250\n",
            " - 16s - loss: 1.7854 - acc: 0.4690\n",
            "Epoch 34/250\n",
            " - 16s - loss: 1.7730 - acc: 0.4729\n",
            "Epoch 35/250\n",
            " - 16s - loss: 1.7596 - acc: 0.4761\n",
            "Epoch 36/250\n",
            " - 16s - loss: 1.7509 - acc: 0.4782\n",
            "Epoch 37/250\n",
            " - 16s - loss: 1.7381 - acc: 0.4804\n",
            "Epoch 38/250\n",
            " - 16s - loss: 1.7271 - acc: 0.4828\n",
            "Epoch 39/250\n",
            " - 16s - loss: 1.7248 - acc: 0.4845\n",
            "Epoch 40/250\n",
            " - 16s - loss: 1.7100 - acc: 0.4900\n",
            "Epoch 41/250\n",
            " - 16s - loss: 1.6995 - acc: 0.4928\n",
            "Epoch 42/250\n",
            " - 16s - loss: 1.6893 - acc: 0.4927\n",
            "Epoch 43/250\n",
            " - 16s - loss: 1.6791 - acc: 0.4971\n",
            "Epoch 44/250\n",
            " - 16s - loss: 1.6696 - acc: 0.4991\n",
            "Epoch 45/250\n",
            " - 16s - loss: 1.6596 - acc: 0.5048\n",
            "Epoch 46/250\n",
            " - 16s - loss: 1.6505 - acc: 0.5036\n",
            "Epoch 47/250\n",
            " - 16s - loss: 1.6404 - acc: 0.5064\n",
            "Epoch 48/250\n",
            " - 17s - loss: 1.6302 - acc: 0.5089\n",
            "Epoch 49/250\n",
            " - 16s - loss: 1.6200 - acc: 0.5138\n",
            "Epoch 50/250\n",
            " - 16s - loss: 1.6126 - acc: 0.5153\n",
            "Epoch 51/250\n",
            " - 16s - loss: 1.6027 - acc: 0.5195\n",
            "Epoch 52/250\n",
            " - 16s - loss: 1.5941 - acc: 0.5177\n",
            "Epoch 53/250\n",
            " - 16s - loss: 1.5830 - acc: 0.5221\n",
            "Epoch 54/250\n",
            " - 16s - loss: 1.5745 - acc: 0.5234\n",
            "Epoch 55/250\n",
            " - 16s - loss: 1.5623 - acc: 0.5284\n",
            "Epoch 56/250\n",
            " - 16s - loss: 1.5528 - acc: 0.5326\n",
            "Epoch 57/250\n",
            " - 16s - loss: 1.5426 - acc: 0.5344\n",
            "Epoch 58/250\n",
            " - 16s - loss: 1.5351 - acc: 0.5340\n",
            "Epoch 59/250\n",
            " - 16s - loss: 1.5241 - acc: 0.5375\n",
            "Epoch 60/250\n",
            " - 16s - loss: 1.5148 - acc: 0.5419\n",
            "Epoch 61/250\n",
            " - 16s - loss: 1.5065 - acc: 0.5433\n",
            "Epoch 62/250\n",
            " - 16s - loss: 1.4960 - acc: 0.5458\n",
            "Epoch 63/250\n",
            " - 16s - loss: 1.4832 - acc: 0.5513\n",
            "Epoch 64/250\n",
            " - 16s - loss: 1.4746 - acc: 0.5499\n",
            "Epoch 65/250\n",
            " - 16s - loss: 1.4634 - acc: 0.5542\n",
            "Epoch 66/250\n",
            " - 16s - loss: 1.4531 - acc: 0.5586\n",
            "Epoch 67/250\n",
            " - 16s - loss: 1.4399 - acc: 0.5635\n",
            "Epoch 68/250\n",
            " - 16s - loss: 1.4278 - acc: 0.5644\n",
            "Epoch 69/250\n",
            " - 16s - loss: 1.4178 - acc: 0.5704\n",
            "Epoch 70/250\n",
            " - 16s - loss: 1.4072 - acc: 0.5690\n",
            "Epoch 71/250\n",
            " - 16s - loss: 1.3965 - acc: 0.5742\n",
            "Epoch 72/250\n",
            " - 16s - loss: 1.3843 - acc: 0.5802\n",
            "Epoch 73/250\n",
            " - 17s - loss: 1.3766 - acc: 0.5791\n",
            "Epoch 74/250\n",
            " - 16s - loss: 1.3627 - acc: 0.5847\n",
            "Epoch 75/250\n",
            " - 16s - loss: 1.3480 - acc: 0.5886\n",
            "Epoch 76/250\n",
            " - 16s - loss: 1.3346 - acc: 0.5933\n",
            "Epoch 77/250\n",
            " - 16s - loss: 1.3235 - acc: 0.5958\n",
            "Epoch 78/250\n",
            " - 16s - loss: 1.3124 - acc: 0.6001\n",
            "Epoch 79/250\n",
            " - 16s - loss: 1.3011 - acc: 0.6030\n",
            "Epoch 80/250\n",
            " - 16s - loss: 1.2859 - acc: 0.6088\n",
            "Epoch 81/250\n",
            " - 16s - loss: 1.2716 - acc: 0.6118\n",
            "Epoch 82/250\n",
            " - 16s - loss: 1.2587 - acc: 0.6161\n",
            "Epoch 83/250\n",
            " - 16s - loss: 1.2491 - acc: 0.6187\n",
            "Epoch 84/250\n",
            " - 16s - loss: 1.2382 - acc: 0.6235\n",
            "Epoch 85/250\n",
            " - 16s - loss: 1.2183 - acc: 0.6312\n",
            "Epoch 86/250\n",
            " - 16s - loss: 1.2025 - acc: 0.6362\n",
            "Epoch 87/250\n",
            " - 17s - loss: 1.1870 - acc: 0.6410\n",
            "Epoch 88/250\n",
            " - 16s - loss: 1.1774 - acc: 0.6417\n",
            "Epoch 89/250\n",
            " - 16s - loss: 1.1659 - acc: 0.6477\n",
            "Epoch 90/250\n",
            " - 16s - loss: 1.1562 - acc: 0.6489\n",
            "Epoch 91/250\n",
            " - 16s - loss: 1.1372 - acc: 0.6560\n",
            "Epoch 92/250\n",
            " - 17s - loss: 1.1218 - acc: 0.6609\n",
            "Epoch 93/250\n",
            " - 16s - loss: 1.1062 - acc: 0.6669\n",
            "Epoch 94/250\n",
            " - 16s - loss: 1.0926 - acc: 0.6714\n",
            "Epoch 95/250\n",
            " - 16s - loss: 1.0712 - acc: 0.6796\n",
            "Epoch 96/250\n",
            " - 16s - loss: 1.0576 - acc: 0.6816\n",
            "Epoch 97/250\n",
            " - 16s - loss: 1.0443 - acc: 0.6847\n",
            "Epoch 98/250\n",
            " - 16s - loss: 1.0314 - acc: 0.6881\n",
            "Epoch 99/250\n",
            " - 16s - loss: 1.0139 - acc: 0.6962\n",
            "Epoch 100/250\n",
            " - 16s - loss: 0.9947 - acc: 0.7039\n",
            "Epoch 101/250\n",
            " - 16s - loss: 0.9856 - acc: 0.7070\n",
            "Epoch 102/250\n",
            " - 16s - loss: 0.9686 - acc: 0.7123\n",
            "Epoch 103/250\n",
            " - 16s - loss: 0.9537 - acc: 0.7187\n",
            "Epoch 104/250\n",
            " - 16s - loss: 0.9355 - acc: 0.7229\n",
            "Epoch 105/250\n",
            " - 16s - loss: 0.9236 - acc: 0.7253\n",
            "Epoch 106/250\n",
            " - 17s - loss: 0.9109 - acc: 0.7331\n",
            "Epoch 107/250\n",
            " - 16s - loss: 0.9004 - acc: 0.7328\n",
            "Epoch 108/250\n",
            " - 16s - loss: 0.8826 - acc: 0.7406\n",
            "Epoch 109/250\n",
            " - 16s - loss: 0.8652 - acc: 0.7485\n",
            "Epoch 110/250\n",
            " - 16s - loss: 0.8545 - acc: 0.7479\n",
            "Epoch 111/250\n",
            " - 17s - loss: 0.8441 - acc: 0.7535\n",
            "Epoch 112/250\n",
            " - 16s - loss: 0.8244 - acc: 0.7586\n",
            "Epoch 113/250\n",
            " - 16s - loss: 0.8093 - acc: 0.7648\n",
            "Epoch 114/250\n",
            " - 16s - loss: 0.7887 - acc: 0.7746\n",
            "Epoch 115/250\n",
            " - 16s - loss: 0.7753 - acc: 0.7778\n",
            "Epoch 116/250\n",
            " - 16s - loss: 0.7657 - acc: 0.7804\n",
            "Epoch 117/250\n",
            " - 16s - loss: 0.7518 - acc: 0.7854\n",
            "Epoch 118/250\n",
            " - 16s - loss: 0.7375 - acc: 0.7882\n",
            "Epoch 119/250\n",
            " - 16s - loss: 0.7336 - acc: 0.7908\n",
            "Epoch 120/250\n",
            " - 16s - loss: 0.7149 - acc: 0.7970\n",
            "Epoch 121/250\n",
            " - 16s - loss: 0.7014 - acc: 0.8010\n",
            "Epoch 122/250\n",
            " - 16s - loss: 0.6843 - acc: 0.8085\n",
            "Epoch 123/250\n",
            " - 16s - loss: 0.6792 - acc: 0.8095\n",
            "Epoch 124/250\n",
            " - 16s - loss: 0.6659 - acc: 0.8123\n",
            "Epoch 125/250\n",
            " - 16s - loss: 0.6681 - acc: 0.8099\n",
            "Epoch 126/250\n",
            " - 17s - loss: 0.6488 - acc: 0.8202\n",
            "Epoch 127/250\n",
            " - 16s - loss: 0.6342 - acc: 0.8229\n",
            "Epoch 128/250\n",
            " - 16s - loss: 0.6182 - acc: 0.8284\n",
            "Epoch 129/250\n",
            " - 16s - loss: 0.6133 - acc: 0.8291\n",
            "Epoch 130/250\n",
            " - 17s - loss: 0.5956 - acc: 0.8357\n",
            "Epoch 131/250\n",
            " - 16s - loss: 0.6128 - acc: 0.8288\n",
            "Epoch 132/250\n",
            " - 16s - loss: 0.5992 - acc: 0.8338\n",
            "Epoch 133/250\n",
            " - 18s - loss: 0.5634 - acc: 0.8486\n",
            "Epoch 134/250\n",
            " - 16s - loss: 0.5573 - acc: 0.8494\n",
            "Epoch 135/250\n",
            " - 16s - loss: 0.5516 - acc: 0.8521\n",
            "Epoch 136/250\n",
            " - 16s - loss: 0.5360 - acc: 0.8553\n",
            "Epoch 137/250\n",
            " - 16s - loss: 0.5270 - acc: 0.8603\n",
            "Epoch 138/250\n",
            " - 16s - loss: 0.5140 - acc: 0.8654\n",
            "Epoch 139/250\n",
            " - 16s - loss: 0.5037 - acc: 0.8674\n",
            "Epoch 140/250\n",
            " - 16s - loss: 0.4985 - acc: 0.8684\n",
            "Epoch 141/250\n",
            " - 16s - loss: 0.5016 - acc: 0.8673\n",
            "Epoch 142/250\n",
            " - 16s - loss: 0.4961 - acc: 0.8695\n",
            "Epoch 143/250\n",
            " - 16s - loss: 0.4888 - acc: 0.8699\n",
            "Epoch 144/250\n",
            " - 16s - loss: 0.4666 - acc: 0.8809\n",
            "Epoch 145/250\n",
            " - 17s - loss: 0.4670 - acc: 0.8788\n",
            "Epoch 146/250\n",
            " - 16s - loss: 0.4587 - acc: 0.8800\n",
            "Epoch 147/250\n",
            " - 16s - loss: 0.4328 - acc: 0.8916\n",
            "Epoch 148/250\n",
            " - 16s - loss: 0.4320 - acc: 0.8911\n",
            "Epoch 149/250\n",
            " - 17s - loss: 0.4299 - acc: 0.8907\n",
            "Epoch 150/250\n",
            " - 16s - loss: 0.4678 - acc: 0.8751\n",
            "Epoch 151/250\n",
            " - 16s - loss: 0.4231 - acc: 0.8927\n",
            "Epoch 152/250\n",
            " - 16s - loss: 0.4024 - acc: 0.9007\n",
            "Epoch 153/250\n",
            " - 16s - loss: 0.4038 - acc: 0.8980\n",
            "Epoch 154/250\n",
            " - 16s - loss: 0.4102 - acc: 0.8933\n",
            "Epoch 155/250\n",
            " - 16s - loss: 0.4096 - acc: 0.8945\n",
            "Epoch 156/250\n",
            " - 16s - loss: 0.3916 - acc: 0.9009\n",
            "Epoch 157/250\n",
            " - 16s - loss: 0.3989 - acc: 0.8998\n",
            "Epoch 158/250\n",
            " - 16s - loss: 0.3799 - acc: 0.9059\n",
            "Epoch 159/250\n",
            " - 16s - loss: 0.3712 - acc: 0.9081\n",
            "Epoch 160/250\n",
            " - 16s - loss: 0.3489 - acc: 0.9182\n",
            "Epoch 161/250\n",
            " - 16s - loss: 0.3373 - acc: 0.9219\n",
            "Epoch 162/250\n",
            " - 16s - loss: 0.3418 - acc: 0.9187\n",
            "Epoch 163/250\n",
            " - 16s - loss: 0.3377 - acc: 0.9202\n",
            "Epoch 164/250\n",
            " - 15s - loss: 0.3411 - acc: 0.9185\n",
            "Epoch 165/250\n",
            " - 17s - loss: 0.3389 - acc: 0.9167\n",
            "Epoch 166/250\n",
            " - 15s - loss: 0.3623 - acc: 0.9081\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1toXrNp8rtMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "modelt1 = load_model(pathname1)\n",
        "modelt2 = load_model(pathname2)\n",
        "modelt3 = load_model(pathname3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HkoAKqNruZ-",
        "colab_type": "code",
        "outputId": "76174a27-e733-4cad-8277-dd08fa2db925",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "def generate_poem(model, mapping, seq_length, seed_text):\n",
        "    in_text = ''\n",
        "    seq = seed_text\n",
        "    newline = 0\n",
        "    while newline < 14:\n",
        "        # encode the characters as integers\n",
        "        encoded = [mapping[char] for char in seq]\n",
        "        # truncate sequences to a fixed length\n",
        "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "        # one hot encode\n",
        "        encoded = to_categorical(encoded[0], num_classes=vocab_size)\n",
        "        encoded = encoded.reshape(1, encoded.shape[0], encoded.shape[1])\n",
        "        # predict character\n",
        "        pred = model.predict_classes(encoded, verbose=0)\n",
        "        # reverse map integer to character and append to input\n",
        "        next_char = r_mapping[pred[0]]\n",
        "        if next_char != '\\n':\n",
        "            in_text += next_char\n",
        "        else:\n",
        "            if (next_char == '\\n' and (len(in_text) > 0 and in_text[len(in_text) - 1] != '\\n')):\n",
        "                in_text += next_char\n",
        "                newline += 1\n",
        "        seq = seq[1:] + next_char\n",
        "    return in_text\n",
        "\n",
        "seed = \"shall i compare thee to a summer's day\\n\"\n",
        "print('Using T = 0.25:')\n",
        "print(generate_poem(modelt1, mapping, 40, seed))\n",
        "print('Using T = 0.75:')\n",
        "print(generate_poem(modelt2, mapping, 40, seed))\n",
        "print('Using T = 1.5:')\n",
        "print(generate_poem(modelt3, mapping, 40, seed))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using T = 0.25:\n",
            "That lle I menoned with succongin do:n:\n",
            "Then haur gaccaan fol of fars sweet love,\n",
            "As ofry them be, asccoun,\n",
            "  And leve's plass ade hllatt timms' with erengne,\n",
            "To sammentel futherent foret disin,\n",
            "My save that bucoonerswith hoo thin my and,\n",
            "  And herv'thess, thence an meatury gand.\n",
            "Whaince that thy coold and thou art and here\n",
            "Whe send the chace con on stcee lled,\n",
            "  Oy love thoe parp thee facceat, of both,\n",
            "  Whe haad that ther freme,\n",
            "The bence forrongrancy and love hatr where\n",
            "Whon the prien of deth thee I bove chnoue.\n",
            "The cack my hat to secas dedw which thee some all,\n",
            "\n",
            "Using T = 0.75:\n",
            "To that mo I con the with my will no prasse\n",
            "For the froce will west batty aw's ruse,\n",
            "Tid if their stray their pastusing ow,\n",
            "Saken in their st be wher\n",
            "s pend at thred by sake me east foust of exert,\n",
            "And merath healt thy seld revenous peest,\n",
            "I fanst when thou dosp ond theis putt seee,\n",
            "Wist the brit love that you grean periee,\n",
            "Theriof thy sedf's conndes in thise can and,\n",
            "Litht thy beart, and beenes stor th ull this,\n",
            "And seapes the beath wost for where brond.\n",
            "  But silf the pist is ast and ald deeats.\n",
            "  Wher olow do so, the sun is love's your.\n",
            "So sos if thy stall of yot thee I mas yough:\n",
            "\n",
            "Using T = 1.5:\n",
            "Why coolt in as the marnst of hearts deel.\n",
            "To timmin swill of cand,\n",
            "Dussakens  sau hath thy fack of hea there\n",
            "For shall subers fave when love ast,aither; thou cand,\n",
            "To by mot eres priss, now nothle mays's sard,\n",
            "And an thrieh thou mestows thou fainst thou maste,\n",
            "  If the raving awonst In a corquich manked,\n",
            "O he forment thee vichorth and shou and sweep;\n",
            "And that then belfole that whetressing mund.\n",
            "  But hold that's so bult int 'tone'st blow, when beinh deading dwall,\n",
            "And love hath my asterit with love shough heart doth tee,\n",
            "Whencence shale thangrns and cand wherefornded,\n",
            "So cards mucessaven to suco awhes thaned,\n",
            "Thoue waith at foot a woorswixt which stell be:\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd6nSVGGY8f1",
        "colab_type": "text"
      },
      "source": [
        "## Additional Goals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yw4M_fAZCgo",
        "colab_type": "text"
      },
      "source": [
        "### Incorporating Additional Texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "serG2Q7gZhGD",
        "colab_type": "text"
      },
      "source": [
        "We can incorporate Spenser's work as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIsFQ96jZFoW",
        "colab_type": "code",
        "outputId": "e51f9db3-86bc-4643-9801-d4a1fa6bc243",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "# Load the data\n",
        "spenser = [w.lower() for w in (open('spenser.txt', 'r').read()).split('\\n')]\n",
        "nltk.download('punkt')\n",
        "nltk.download('cmudict')\n",
        "\n",
        "# Use the CMUs Pronouncing Dictionary\n",
        "from nltk.corpus import cmudict\n",
        "d = cmudict.dict()\n",
        "\n",
        "# Add on to our Shakespeare lists\n",
        "grammar = [\",\", \";\", \".\", \":\", \"?\", \"(\", \")\", \"!\"]\n",
        "full_word_list = word_list.copy()\n",
        "full_syll_dict = syll_dict.copy()\n",
        "\n",
        "for i in range(len(spenser)):\n",
        "  if len(spenser[i]) > len(spenser[0]) + 10: # don't read the number lines\n",
        "    seq = spenser[i].replace(\"-\", \" \")\n",
        "    line = [w for w in nltk.word_tokenize(seq) if w not in grammar and w.isalpha()]\n",
        "    if all(w in d for w in line):\n",
        "      for w in line:\n",
        "        if w not in full_syll_dict:\n",
        "          syllables = [len(list(y for y in x if y[-1].isdigit())) for x in d[w]][0]\n",
        "          full_syll_dict[w] = syllables\n",
        "      full_word_list.extend(line)\n",
        "\n",
        "full_word_list = list(set(full_word_list))\n",
        "full_num_words = len(full_word_list)\n",
        "\n",
        "# Create dictionaries of the words to give them index values for easier training\n",
        "full_word_int = {word:i for i, word in enumerate(full_word_list)}\n",
        "full_int_word = {i:word for i, word in enumerate(full_word_list)}\n",
        "\n",
        "# Create training data\n",
        "full_train_X = train_X.copy()\n",
        "\n",
        "for i in range(len(spenser)):\n",
        "  if len(spenser[i]) > len(spenser[0]) + 10: # don't read the number lines\n",
        "    seq = spenser[i].replace(\"-\", \" \")\n",
        "    line = [w for w in nltk.word_tokenize(seq) if w not in grammar and w.isalpha()]\n",
        "    if all(w in full_word_list for w in line):\n",
        "      train_line = [int(full_word_int[w]) for w in line]\n",
        "      full_train_X.append(train_line)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Package cmudict is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3txbucH2Y98n",
        "colab_type": "code",
        "outputId": "29d74a1f-c15d-4cf8-d162-8c2ed25db0bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Unsupervised HMM learninig\n",
        "n_states = 10\n",
        "n_iters = 100\n",
        "full_HMM = unsupervised_HMM(full_train_X, n_states, n_iters)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:166: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOby7-0I_6sQ",
        "colab_type": "code",
        "outputId": "da2b4ccd-c012-44cd-b00c-157b9b898e03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "sequence_generator(full_HMM, 14, 10, syllable_dict=full_syll_dict, int_dict=full_int_word)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " lack diseased hands am might stores part fortune\n",
            " shown lack stores tires learning o'erlook world which\n",
            " choked determined dressed spur short-numbered\n",
            " chain but meat do of gainer mind strangely\n",
            " still view hot herself cheater learned figures\n",
            " losing jewels baser devil inspire\n",
            " entice olives am washed strangely tract dove\n",
            " tires could asked love cherish woo dies avenge\n",
            " mightst her diseased clogged betraying stores feeds\n",
            " delivers was situation doth they\n",
            " sad razed runs strangely gainer strife read\n",
            " learning and modern a visage at hand\n",
            " in graciously end within gainer but\n",
            " penury cordials separable her\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiPvVbPUPBbO",
        "colab_type": "text"
      },
      "source": [
        "### Incorporating Rhyme"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7BicYiyXQPQ",
        "colab_type": "text"
      },
      "source": [
        "We first create a rhyming dictionary from Shakespeare's sonnets and then generate emissions backwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUWblC2o9YJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ya-ryvokPFD-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initiliaze\n",
        "rhyme_dict = {}\n",
        "\n",
        "# Keep track of how many lines we've gone through\n",
        "count, l = 0, 1\n",
        "a = []\n",
        "b = []\n",
        "last = False\n",
        "grammar.append('\\'')\n",
        "\n",
        "# Unfortunately poem 99 and 126 do not follow the usual pattern so we need\n",
        "# to handle these poems separately\n",
        "skip = False\n",
        "\n",
        "# Go through each abab line scheme and add to our rhyming dictionary\n",
        "for i in range(len(shakespeare)):\n",
        "  # Why did Shakespeare create two poems that are irregular to his usual\n",
        "  # style? The world will never know.\n",
        "  if shakespeare[i].strip() == \"99\" or shakespeare[i].strip() == \"126\":\n",
        "    skip = True\n",
        "  if shakespeare[i].strip() == \"100\" or shakespeare[i].strip() == \"127\":\n",
        "    skip = False\n",
        "  if not skip:\n",
        "    if len(shakespeare[i]) > len(shakespeare[0]) + 2: # don't read the number lines\n",
        "      line = [w for w in nltk.word_tokenize(shakespeare[i]) if w not in grammar]\n",
        "      \n",
        "      # If we are at the end of the poem\n",
        "      if (l + 1) % 14 == 0 and l != 0:\n",
        "        last = True\n",
        "        word = line[-1] if line[-1] not in grammar else line[-2]\n",
        "        a.append(word)\n",
        "      elif last:\n",
        "        word = line[-1] if line[-1] not in grammar else line[-2]\n",
        "        a.append(word)\n",
        "        last = False\n",
        "      else:\n",
        "        if l % 2 == 0:\n",
        "          word = line[-1] if line[-1] not in grammar else line[-2]\n",
        "          a.append(word)\n",
        "        else:\n",
        "          word = line[-1] if line[-1] not in grammar else line[-2]\n",
        "          b.append(word)\n",
        "        \n",
        "      if len(a) == 2:\n",
        "        rhyme_dict[count] = a\n",
        "        count += 1\n",
        "        a = []\n",
        "      if len(b) == 2:\n",
        "        rhyme_dict[count] = b\n",
        "        count += 1\n",
        "        b = []\n",
        "      l += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3DYPL6dszu-",
        "colab_type": "code",
        "outputId": "e081d9c5-d0e0-4076-efa0-15ea298de088",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(rhyme_dict)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: ['increase', 'decease'], 1: ['die', 'memory'], 2: ['eyes', 'lies'], 3: ['fuel', 'cruel'], 4: ['ornament', 'content'], 5: ['spring', 'niggarding'], 6: ['be', 'thee'], 7: ['brow', 'now'], 8: ['field', 'held'], 9: ['lies', 'eyes'], 10: ['days', 'praise'], 11: ['use', 'excuse'], 12: ['mine', 'thine'], 13: ['old', 'cold'], 14: ['viewest', 'renewest'], 15: ['another', 'mother'], 16: ['womb', 'tomb'], 17: ['husbandry', 'posterity'], 18: ['thee', 'see'], 19: ['prime', 'time'], 20: ['be', 'thee'], 21: ['spend', 'lend'], 22: ['legacy', 'free'], 23: ['abuse', 'use'], 24: ['give', 'live'], 25: ['alone', 'gone'], 26: ['deceive', 'leave'], 27: ['thee', 'be'], 28: ['frame', 'same'], 29: ['dwell', 'excel'], 30: ['on', 'gone'], 31: ['there', 'where'], 32: ['left', 'bereft'], 33: ['glass', 'was'], 34: ['meet', 'sweet'], 35: ['deface', 'place'], 36: ['distilled', 'self-killed'], 37: ['usury', 'thee'], 38: ['loan', 'one'], 39: ['art', 'depart'], 40: ['thee', 'posterity'], 41: ['fair', 'heir'], 42: ['light', 'sight'], 43: ['eye', 'majesty'], 44: ['hill', 'still'], 45: ['age', 'pilgrimage'], 46: ['car', 'are'], 47: ['day', 'way'], 48: ['noon', 'son'], 49: ['sadly', 'gladly'], 50: ['joy', 'annoy'], 51: ['sounds', 'confounds'], 52: ['ear', 'bear'], 53: ['another', 'mother'], 54: ['ordering', 'sing'], 55: ['one', 'none'], 56: ['eye', 'die'], 57: ['life', 'wife'], 58: ['weep', 'keep'], 59: ['behind', 'mind'], 60: ['spend', 'end'], 61: ['it', 'it'], 62: ['sits', 'commits'], 63: ['any', 'many'], 64: ['unprovident', 'evident'], 65: ['hate', 'ruinate'], 66: ['conspire', 'desire'], 67: ['mind', 'kind'], 68: ['love', 'prove'], 69: ['me', 'thee'], 70: [\"grow'st\", \"bestow'st\"], 71: ['departest', 'convertest'], 72: ['increase', 'cease'], 73: ['decay', 'away'], 74: ['store', 'more'], 75: ['perish', 'cherish'], 76: ['thereby', 'die'], 77: ['time', 'prime'], 78: ['night', 'white'], 79: ['leaves', 'sheaves'], 80: ['herd', 'beard'], 81: ['make', 'forsake'], 82: ['go', 'grow'], 83: ['defence', 'hence'], 84: ['are', 'prepare'], 85: ['live', 'give'], 86: ['lease', 'decease'], 87: ['were', 'bear'], 88: ['decay', 'day'], 89: ['uphold', 'cold'], 90: ['know', 'so'], 91: ['pluck', 'luck'], 92: ['astronomy', 'quality'], 93: ['tell', 'well'], 94: ['wind', 'find'], 95: ['derive', 'thrive'], 96: ['art', 'convert'], 97: ['prognosticate', 'date'], 98: ['grows', 'shows'], 99: ['moment', 'comment'], 100: ['increase', 'decrease'], 101: ['sky', 'memory'], 102: ['stay', 'decay'], 103: ['sight', 'night'], 104: ['you', 'new'], 105: ['way', 'decay'], 106: ['time', 'rhyme'], 107: ['hours', 'flowers'], 108: ['unset', 'counterfeit'], 109: ['repair', 'fair'], 110: ['pen', 'men'], 111: ['still', 'skill'], 112: ['come', 'tomb'], 113: ['deserts', 'parts'], 114: ['eyes', 'lies'], 115: ['graces', 'faces'], 116: ['age', 'rage'], 117: ['tongue', 'song'], 118: ['time', 'rhyme'], 119: ['day', 'may'], 120: ['temperate', 'date'], 121: ['shines', 'declines'], 122: ['dimmed', 'untrimmed'], 123: ['fade', 'shade'], 124: [\"ow'st\", \"grow'st\"], 125: ['see', 'thee'], 126: ['paws', 'jaws'], 127: ['brood', 'blood'], 128: [\"fleet'st\", 'sweets'], 129: ['time', 'crime'], 130: ['brow', 'allow'], 131: ['pen', 'men'], 132: ['wrong', 'young'], 133: ['painted', 'acquainted'], 134: ['passion', 'fashion'], 135: ['rolling', 'controlling'], 136: ['gazeth', 'amazeth'], 137: ['created', 'defeated'], 138: ['a-doting', 'nothing'], 139: ['pleasure', 'treasure'], 140: ['muse', 'use'], 141: ['verse', 'rehearse'], 142: ['compare', 'rare'], 143: ['gems', 'hems'], 144: ['write', 'bright'], 145: ['fair', 'air'], 146: ['well', 'sell'], 147: ['old', 'behold'], 148: ['date', 'expiate'], 149: ['thee', 'me'], 150: ['heart', 'art'], 151: ['wary', 'chary'], 152: ['will', 'ill'], 153: ['slain', 'again'], 154: ['stage', 'rage'], 155: ['part', 'heart'], 156: ['say', 'decay'], 157: ['rite', 'might'], 158: ['eloquence', 'recompense'], 159: ['breast', 'expressed'], 160: ['writ', 'wit'], 161: ['stelled', 'held'], 162: ['heart', 'art'], 163: ['skill', 'still'], 164: ['lies', 'eyes'], 165: ['done', 'sun'], 166: ['me', 'thee'], 167: ['art', 'heart'], 168: ['stars', 'bars'], 169: ['boast', 'most'], 170: ['spread', 'buried'], 171: ['eye', 'die'], 172: ['fight', 'quite'], 173: ['foiled', 'toiled'], 174: ['beloved', 'removed'], 175: ['vassalage', 'embassage'], 176: ['knit', 'wit'], 177: ['mine', 'thine'], 178: ['it', 'it'], 179: ['moving', 'loving'], 180: ['aspect', 'respect'], 181: ['thee', 'me'], 182: ['bed', 'head'], 183: ['tired', 'expired'], 184: ['abide', 'wide'], 185: ['thee', 'see'], 186: ['sight', 'night'], 187: ['view', 'new'], 188: ['mind', 'find'], 189: ['plight', 'night'], 190: ['rest', 'oppressed'], 191: ['reign', 'complain'], 192: ['me', 'thee'], 193: ['bright', 'night'], 194: ['heaven', 'even'], 195: ['longer', 'stronger'], 196: ['eyes', 'cries'], 197: ['state', 'fate'], 198: ['hope', 'scope'], 199: ['possessed', 'least'], 200: ['despising', 'arising'], 201: ['state', 'gate'], 202: ['brings', 'kings'], 203: ['thought', 'sought'], 204: ['past', 'waste'], 205: ['flow', 'woe'], 206: ['night', 'sight'], 207: ['foregone', 'moan'], 208: [\"o'er\", 'before'], 209: ['friend', 'end'], 210: ['hearts', 'parts'], 211: ['dead', 'buried'], 212: ['tear', 'appear'], 213: ['eye', 'lie'], 214: ['live', 'give'], 215: ['gone', 'alone'], 216: ['thee', 'me'], 217: ['day', 're-survey'], 218: ['cover', 'lover'], 219: ['time', 'rhyme'], 220: ['pen', 'men'], 221: ['thought', 'brought'], 222: ['age', 'equipage'], 223: ['prove', 'love'], 224: ['seen', 'green'], 225: ['eye', 'alchemy'], 226: ['ride', 'hide'], 227: ['face', 'disgrace'], 228: ['shine', 'mine'], 229: ['brow', 'now'], 230: ['disdaineth', 'staineth'], 231: ['day', 'way'], 232: ['cloak', 'smoke'], 233: ['break', 'speak'], 234: ['face', 'disgrace'], 235: ['grief', 'relief'], 236: ['loss', 'cross'], 237: ['sheds', 'deeds'], 238: ['done', 'sun'], 239: ['mud', 'bud'], 240: ['this', 'amiss'], 241: ['compare', 'are'], 242: ['sense', 'commence'], 243: ['advocate', 'hate'], 244: ['be', 'me'], 245: ['twain', 'remain'], 246: ['one', 'alone'], 247: ['respect', 'effect'], 248: ['spite', 'delight'], 249: ['thee', 'me'], 250: ['shame', 'name'], 251: ['sort', 'report'], 252: ['delight', 'spite'], 253: ['youth', 'truth'], 254: ['wit', 'sit'], 255: ['more', 'store'], 256: ['despised', 'sufficed'], 257: ['give', 'live'], 258: ['thee', 'me'], 259: ['invent', 'excellent'], 260: ['verse', 'rehearse'], 261: ['me', 'thee'], 262: ['sight', 'light'], 263: ['worth', 'forth'], 264: ['invocate', 'date'], 265: ['days', 'praise'], 266: ['sing', 'bring'], 267: ['me', 'thee'], 268: ['live', 'give'], 269: ['one', 'alone'], 270: ['prove', 'love'], 271: ['leave', 'deceive'], 272: ['twain', 'remain'], 273: ['all', 'call'], 274: ['before', 'more'], 275: ['receivest', 'deceivest'], 276: ['usest', 'refusest'], 277: ['thief', 'grief'], 278: ['poverty', 'injury'], 279: ['shows', 'foes'], 280: ['commits', 'befits'], 281: ['heart', 'art'], 282: ['won', 'son'], 283: ['assailed', 'prevailed'], 284: ['forbear', 'there'], 285: ['youth', 'truth'], 286: ['thee', 'me'], 287: ['grief', 'chief'], 288: ['dearly', 'nearly'], 289: ['ye', 'me'], 290: ['her', 'her'], 291: ['gain', 'twain'], 292: ['loss', 'cross'], 293: ['one', 'alone'], 294: ['see', 'thee'], 295: ['unrespected', 'directed'], 296: ['bright', 'light'], 297: ['show', 'so'], 298: ['made', 'shade'], 299: ['day', 'stay'], 300: ['thee', 'me'], 301: ['thought', 'brought'], 302: ['way', 'stay'], 303: ['stand', 'land'], 304: ['thee', 'be'], 305: ['thought', 'wrought'], 306: ['gone', 'moan'], 307: ['slow', 'woe'], 308: ['fire', 'desire'], 309: ['abide', 'slide'], 310: ['gone', 'alone'], 311: ['thee', 'melancholy'], 312: ['recured', 'assured'], 313: ['thee', 'me'], 314: ['glad', 'sad'], 315: ['war', 'bar'], 316: ['sight', 'right'], 317: ['lie', 'deny'], 318: ['eyes', 'lies'], 319: ['impanelled', 'determined'], 320: ['heart', 'part'], 321: ['part', 'heart'], 322: ['took', 'look'], 323: ['other', 'smother'], 324: ['feast', 'guest'], 325: ['heart', 'part'], 326: ['love', 'move'], 327: ['me', 'thee'], 328: ['sight', 'delight'], 329: ['way', 'stay'], 330: ['thrust', 'trust'], 331: ['are', 'care'], 332: ['grief', 'thief'], 333: ['chest', 'breast'], 334: ['art', 'part'], 335: ['fear', 'dear'], 336: ['come', 'sum'], 337: ['defects', 'respects'], 338: ['pass', 'was'], 339: ['eye', 'gravity'], 340: ['here', 'uprear'], 341: ['desert', 'part'], 342: ['laws', 'cause'], 343: ['way', 'say'], 344: ['end', 'friend'], 345: ['woe', 'know'], 346: ['me', 'thee'], 347: ['on', 'groan'], 348: ['hide', 'side'], 349: ['mind', 'behind'], 350: ['offence', 'thence'], 351: ['speed', 'need'], 352: ['find', 'wind'], 353: ['slow', 'know'], 354: ['pace', 'race'], 355: ['made', 'jade'], 356: ['wilful-slow', 'go'], 357: ['key', 'survey'], 358: ['treasure', 'pleasure'], 359: ['rare', 'are'], 360: ['set', 'carcanet'], 361: ['chest', 'special-blest'], 362: ['hide', 'pride'], 363: ['scope', 'hope'], 364: ['made', 'shade'], 365: ['tend', 'lend'], 366: ['counterfeit', 'set'], 367: ['you', 'new'], 368: ['year', 'appear'], 369: ['show', 'know'], 370: ['part', 'heart'], 371: ['seem', 'deem'], 372: ['give', 'live'], 373: ['dye', 'wantonly'], 374: ['roses', 'discloses'], 375: ['show', 'so'], 376: ['fade', 'made'], 377: ['youth', 'truth'], 378: ['monuments', 'contents'], 379: ['rhyme', 'time'], 380: ['overturn', 'burn'], 381: ['masonry', 'memory'], 382: ['enmity', 'posterity'], 383: ['room', 'doom'], 384: ['arise', 'eyes'], 385: ['said', 'allayed'], 386: ['appetite', 'might'], 387: ['fill', 'kill'], 388: ['fulness', 'dulness'], 389: ['be', 'see'], 390: ['new', 'view'], 391: ['care', 'rare'], 392: ['tend', 'spend'], 393: ['desire', 'require'], 394: ['hour', 'sour'], 395: ['you', 'adieu'], 396: ['thought', 'nought'], 397: ['suppose', 'those'], 398: ['will', 'ill'], 399: ['slave', 'crave'], 400: ['pleasure', 'leisure'], 401: ['beck', 'check'], 402: ['liberty', 'injury'], 403: ['strong', 'belong'], 404: ['time', 'crime'], 405: ['hell', 'well'], 406: ['is', 'amis'], 407: ['beguiled', 'child'], 408: ['look', 'book'], 409: ['sun', 'done'], 410: ['say', 'they'], 411: ['frame', 'same'], 412: ['days', 'praise'], 413: ['shore', 'before'], 414: ['end', 'contend'], 415: ['light', 'fight'], 416: ['crowned', 'confound'], 417: ['youth', 'truth'], 418: ['brow', 'mow'], 419: ['stand', 'hand'], 420: ['open', 'broken'], 421: ['night', 'sight'], 422: ['thee', 'me'], 423: ['pry', 'jealousy'], 424: ['great', 'defeat'], 425: ['awake', 'sake'], 426: ['elsewhere', 'near'], 427: ['eye', 'remedy'], 428: ['part', 'heart'], 429: ['mine', 'define'], 430: ['account', 'surmount'], 431: ['indeed', 'read'], 432: ['antiquity', 'iniquity'], 433: ['praise', 'days'], 434: ['now', 'brow'], 435: [\"o'erworn\", 'morn'], 436: ['night', 'sight'], 437: ['king', 'spring'], 438: ['fortify', 'memory'], 439: ['knife', 'life'], 440: ['seen', 'green'], 441: ['defaced', 'down-rased'], 442: ['age', 'rage'], 443: ['gain', 'main'], 444: ['shore', 'store'], 445: ['state', 'ruminate'], 446: ['decay', 'away'], 447: ['choose', 'lose'], 448: ['sea', 'plea'], 449: ['power', 'flower'], 450: ['out', 'stout'], 451: ['days', 'decays'], 452: ['alack', 'back'], 453: ['hid', 'forbid'], 454: ['might', 'bright'], 455: ['cry', 'jollity'], 456: ['born', 'forsworn'], 457: ['misplaced', 'disgraced'], 458: ['strumpeted', 'disabled'], 459: ['authority', 'simplicity'], 460: ['skill', 'ill'], 461: ['gone', 'alone'], 462: ['live', 'achieve'], 463: ['impiety', 'society'], 464: ['cheek', 'seek'], 465: ['hue', 'true'], 466: ['is', 'his'], 467: ['veins', 'gains'], 468: ['had', 'bad'], 469: ['outworn', 'born'], 470: ['now', 'brow'], 471: ['dead', 'head'], 472: ['away', 'gay'], 473: ['seen', 'green'], 474: ['true', 'new'], 475: ['store', 'yore'], 476: ['view', 'due'], 477: ['mend', 'commend'], 478: ['crowned', 'confound'], 479: ['own', 'shown'], 480: ['mind', 'kind'], 481: ['deeds', 'weeds'], 482: ['show', 'grow'], 483: ['defect', 'suspect'], 484: ['fair', 'air'], 485: ['approve', 'love'], 486: ['time', 'prime'], 487: ['days', 'praise'], 488: ['charged', 'enlarged'], 489: ['show', 'owe'], 490: ['dead', 'fled'], 491: ['bell', 'dwell'], 492: ['not', 'forgot'], 493: ['so', 'woe'], 494: ['verse', 'rehearse'], 495: ['clay', 'decay'], 496: ['moan', 'gone'], 497: ['recite', 'quite'], 498: ['love', 'prove'], 499: ['lie', 'i'], 500: ['desert', 'impart'], 501: ['this', 'is'], 502: ['untrue', 'you'], 503: ['forth', 'worth'], 504: ['behold', 'cold'], 505: ['hang', 'sang'], 506: ['day', 'away'], 507: ['west', 'rest'], 508: ['fire', 'expire'], 509: ['lie', 'by'], 510: ['strong', 'long'], 511: ['arrest', 'interest'], 512: ['away', 'stay'], 513: ['review', 'due'], 514: ['thee', 'me'], 515: ['life', 'knife'], 516: ['dead', 'remembered'], 517: ['contains', 'remains'], 518: ['life', 'strife'], 519: ['ground', 'found'], 520: ['anon', 'alone'], 521: ['treasure', 'pleasure'], 522: ['sight', 'delight'], 523: ['look', 'took'], 524: ['day', 'away'], 525: ['pride', 'aside'], 526: ['change', 'strange'], 527: ['same', 'name'], 528: ['weed', 'proceed'], 529: ['you', 'new'], 530: ['argument', 'spent'], 531: ['old', 'told'], 532: ['wear', 'bear'], 533: ['waste', 'taste'], 534: ['show', 'know'], 535: ['memory', 'eternity'], 536: ['contain', 'brain'], 537: ['find', 'mind'], 538: ['look', 'book'], 539: ['muse', 'use'], 540: ['verse', 'disperse'], 541: ['sing', 'wing'], 542: ['fly', 'majesty'], 543: ['compile', 'style'], 544: ['thee', 'be'], 545: ['advance', 'ignorance'], 546: ['aid', 'decayed'], 547: ['grace', 'place'], 548: ['argument', 'invent'], 549: ['pen', 'again'], 550: ['word', 'afford'], 551: ['give', 'live'], 552: ['say', 'pay'], 553: ['write', 'might'], 554: ['name', 'fame'], 555: ['is', 'his'], 556: ['bear', 'appear'], 557: ['afloat', 'boat'], 558: ['ride', 'pride'], 559: ['away', 'decay'], 560: ['make', 'take'], 561: ['rotten', 'forgotten'], 562: ['have', 'grave'], 563: ['die', 'lie'], 564: ['verse', 'rehearse'], 565: [\"o'er-read\", 'dead'], 566: ['pen', 'men'], 567: ['muse', 'use'], 568: [\"o'erlook\", 'book'], 569: ['hue', 'anew'], 570: ['praise', 'days'], 571: ['devised', 'sympathized'], 572: ['lend', 'friend'], 573: ['used', 'abused'], 574: ['need', 'exceed'], 575: ['set', 'debt'], 576: ['report', 'short'], 577: ['show', 'grow'], 578: ['impute', 'mute'], 579: ['dumb', 'tomb'], 580: ['eyes', 'devise'], 581: ['more', 'store'], 582: ['you', 'grew'], 583: ['dwell', 'tell'], 584: ['glory', 'story'], 585: ['writ', 'wit'], 586: ['clear', 'where'], 587: ['curse', 'worse'], 588: ['still', 'quill'], 589: ['compiled', 'filed'], 590: ['words', 'affords'], 591: ['amen', 'pen'], 592: ['true', 'you'], 593: ['more', 'before'], 594: ['respect', 'effect'], 595: ['verse', 'inhearse'], 596: ['you', 'grew'], 597: ['write', 'night'], 598: ['dead', 'astonished'], 599: ['ghost', 'boast'], 600: ['intelligence', 'thence'], 601: ['line', 'mine'], 602: ['possessing', 'releasing'], 603: ['estimate', 'determinate'], 604: ['granting', 'wanting'], 605: ['deserving', 'swerving'], 606: ['knowing', 'growing'], 607: ['mistaking', 'making'], 608: ['flatter', 'matter'], 609: ['light', 'fight'], 610: ['scorn', 'forsworn'], 611: ['acquainted', 'attainted'], 612: ['story', 'glory'], 613: ['too', 'do'], 614: ['thee', 'me'], 615: ['belong', 'wrong'], 616: ['fault', 'halt'], 617: ['offence', 'defence'], 618: ['ill', 'will'], 619: ['change', 'strange'], 620: ['tongue', 'wronk'], 621: ['dwell', 'tell'], 622: ['debate', 'hate'], 623: ['now', 'bow'], 624: ['cross', 'after-loss'], 625: ['sorrow', 'morrow'], 626: ['woe', 'overthrow'], 627: ['last', 'taste'], 628: ['spite', 'might'], 629: ['woe', 'so'], 630: ['skill', 'ill'], 631: ['force', 'horse'], 632: ['pleasure', 'measure'], 633: ['rest', 'best'], 634: ['me', 'be'], 635: ['costs', 'boast'], 636: ['take', 'make'], 637: ['away', 'stay'], 638: ['mine', 'thine'], 639: ['wrongs', 'belongs'], 640: ['end', 'depend'], 641: ['mind', 'find'], 642: ['lie', 'die'], 643: ['blot', 'not'], 644: ['true', 'new'], 645: ['face', 'place'], 646: ['eye', 'history'], 647: ['change', 'strange'], 648: ['decree', 'be'], 649: ['dwell', 'tell'], 650: ['grow', 'show'], 651: ['none', 'stone'], 652: ['show', 'slow'], 653: ['graces', 'faces'], 654: ['expense', 'excellence'], 655: ['sweet', 'meet'], 656: ['die', 'dignity'], 657: ['deeds', 'weeds'], 658: ['shame', 'name'], 659: ['rose', 'enclose'], 660: ['days', 'praise'], 661: ['sport', 'report'], 662: ['got', 'blot'], 663: ['thee', 'see'], 664: ['privilege', 'edge'], 665: ['wantonness', 'less'], 666: ['sport', 'resort'], 667: ['queen', 'seen'], 668: ['esteemed', 'deemed'], 669: ['betray', 'away'], 670: ['translate', 'state'], 671: ['sort', 'report'], 672: ['been', 'seen'], 673: ['year', 'everywhere'], 674: ['time', 'prime'], 675: ['increase', 'decease'], 676: ['me', 'thee'], 677: ['fruit', 'mute'], 678: ['cheer', 'near'], 679: ['spring', 'thing'], 680: ['trim', 'him'], 681: ['smell', 'tell'], 682: ['hue', 'grew'], 683: ['white', 'delight'], 684: ['rose', 'those'], 685: ['away', 'play'], 686: ['long', 'song'], 687: ['might', 'light'], 688: ['redeem', 'esteem'], 689: ['spent', 'argument'], 690: ['survey', 'decay'], 691: ['there', 'everywhere'], 692: ['life', 'knife'], 693: ['amends', 'depends'], 694: ['dyed', 'dignified'], 695: ['say', 'lay'], 696: ['fixed', 'intermixed'], 697: ['dumb', 'tomb'], 698: ['thee', 'be'], 699: ['how', 'now'], 700: ['seeming', 'esteeming'], 701: ['appear', 'where'], 702: ['spring', 'sing'], 703: ['lays', 'days'], 704: ['now', 'bough'], 705: ['night', 'delight'], 706: ['tongue', 'song'], 707: ['forth', 'worth'], 708: ['pride', 'beside'], 709: ['write', 'quite'], 710: ['face', 'disgrace'], 711: ['mend', 'tend'], 712: ['well', 'tell'], 713: ['sit', 'it'], 714: ['old', 'cold'], 715: ['eyed', 'pride'], 716: ['turned', 'burned'], 717: ['seen', 'green'], 718: ['hand', 'stand'], 719: ['perceived', 'deceived'], 720: ['unbred', 'dead'], 721: ['idolatry', 'be'], 722: ['show', 'so'], 723: ['kind', 'confined'], 724: ['excellence', 'difference'], 725: ['argument', 'spent'], 726: ['words', 'affords'], 727: ['alone', 'one'], 728: ['time', 'rhyme'], 729: ['wights', 'knights'], 730: ['best', 'expressed'], 731: ['brow', 'now'], 732: ['prophecies', 'eyes'], 733: ['prefiguring', 'sing'], 734: ['days', 'praise'], 735: ['soul', 'control'], 736: ['come', 'doom'], 737: ['endured', 'assured'], 738: ['presage', 'age'], 739: ['time', 'rhyme'], 740: ['subscribes', 'tribes'], 741: ['monument', 'spent'], 742: ['character', 'register'], 743: ['spirit', 'merit'], 744: ['divine', 'thine'], 745: ['same', 'name'], 746: ['case', 'place'], 747: ['age', 'page'], 748: ['bred', 'dead'], 749: ['heart', 'depart'], 750: ['qualify', 'lie'], 751: ['ranged', 'exchanged'], 752: ['again', 'stain'], 753: ['reigned', 'stained'], 754: ['blood', 'good'], 755: ['call', 'all'], 756: ['there', 'dear'], 757: ['view', 'new'], 758: ['truth', 'youth'], 759: ['above', 'love'], 760: ['end', 'friend'], 761: ['grind', 'confined'], 762: ['best', 'breast'], 763: ['chide', 'provide'], 764: ['deeds', 'breeds'], 765: ['brand', 'hand'], 766: ['subdued', 'renewed'], 767: ['drink', 'think'], 768: ['infection', 'correction'], 769: ['ye', 'me'], 770: ['fill', 'ill'], 771: ['brow', 'allow'], 772: ['strive', 'alive'], 773: ['tongue', 'wrong'], 774: ['care', 'are'], 775: ['sense', 'dispense'], 776: ['bred', 'dead'], 777: ['mind', 'blind'], 778: ['about', 'out'], 779: ['heart', 'part'], 780: ['latch', 'catch'], 781: ['sight', 'night'], 782: ['creature', 'feature'], 783: ['you', 'untrue'], 784: ['you', 'true'], 785: ['flattery', 'alchemy'], 786: ['indigest', 'best'], 787: ['resemble', 'assemble'], 788: ['seeing', \"'greeing\"], 789: ['up', 'cup'], 790: ['sin', 'begin'], 791: ['lie', 'why'], 792: ['dearer', 'clearer'], 793: ['accidents', 'intents'], 794: ['kings', 'things'], 795: ['tyranny', 'incertainty'], 796: ['best', 'rest'], 797: ['so', 'grow'], 798: ['minds', 'finds'], 799: ['love', 'remove'], 800: ['mark', 'bark'], 801: ['shaken', 'taken'], 802: ['cheeks', 'weeks'], 803: ['come', 'doom'], 804: ['proved', 'loved'], 805: ['all', 'call'], 806: ['repay', 'day'], 807: ['minds', 'winds'], 808: ['right', 'sight'], 809: ['down', 'frown'], 810: ['accumulate', 'hate'], 811: ['prove', 'love'], 812: ['keen', 'unseen'], 813: ['urge', 'purge'], 814: ['sweetness', 'meetness'], 815: ['feeding', 'needing'], 816: ['anticipate', 'state'], 817: ['assured', 'cured'], 818: ['true', 'you'], 819: ['tears', 'fears'], 820: ['within', 'win'], 821: ['committed', 'fitted'], 822: ['never', 'fever'], 823: ['true', 'anew'], 824: ['better', 'greater'], 825: ['content', 'spent'], 826: ['now', 'bow'], 827: ['feel', 'steel'], 828: ['shaken', 'taken'], 829: ['time', 'crime'], 830: ['remembered', 'tendered'], 831: ['hits', 'fits'], 832: ['fee', 'me'], 833: ['esteemed', 'deemed'], 834: ['being', 'seeing'], 835: ['eyes', 'spies'], 836: ['blood', 'good'], 837: ['level', 'bevel'], 838: ['own', 'shown'], 839: ['maintain', 'reign'], 840: ['brain', 'remain'], 841: ['memory', 'eternity'], 842: ['heart', 'part'], 843: ['subsist', 'missed'], 844: ['hold', 'bold'], 845: ['score', 'more'], 846: ['thee', 'me'], 847: ['change', 'strange'], 848: ['might', 'sight'], 849: ['admire', 'desire'], 850: ['old', 'told'], 851: ['defy', 'lie'], 852: ['past', 'haste'], 853: ['be', 'thee'], 854: ['state', 'hate'], 855: ['unfathered', 'gathered'], 856: ['accident', 'discontent'], 857: ['falls', 'calls'], 858: ['heretic', 'politic'], 859: ['hours', 'showers'], 860: ['time', 'crime'], 861: ['canopy', 'eternity'], 862: ['honouring', 'ruining'], 863: ['favour', 'savour'], 864: ['rent', 'spent'], 865: ['heart', 'art'], 866: ['free', 'thee'], 867: ['soul', 'control'], 868: ['fair', 'heir'], 869: ['name', 'shame'], 870: ['power', 'bower'], 871: ['face', 'disgrace'], 872: ['black', 'lack'], 873: ['seem', 'esteem'], 874: ['woe', 'so'], 875: [\"play'st\", \"sway'st\"], 876: ['sounds', 'confounds'], 877: ['leap', 'reap'], 878: ['hand', 'stand'], 879: ['state', 'gait'], 880: ['chips', 'lips'], 881: ['this', 'kiss'], 882: ['shame', 'blame'], 883: ['lust', 'trust'], 884: ['straight', 'bait'], 885: ['had', 'mad'], 886: ['so', 'woe'], 887: ['extreme', 'dream'], 888: ['well', 'hell'], 889: ['sun', 'dun'], 890: ['red', 'head'], 891: ['white', 'delight'], 892: ['cheeks', 'reeks'], 893: ['know', 'go'], 894: ['sound', 'ground'], 895: ['rare', 'compare'], 896: ['art', 'heart'], 897: ['cruel', 'jewel'], 898: ['behold', 'bold'], 899: ['groan', 'alone'], 900: ['swear', 'bear'], 901: ['face', 'place'], 902: ['deeds', 'proceeds'], 903: ['me', 'be'], 904: ['disdain', 'pain'], 905: ['heaven', 'even'], 906: ['east', 'west'], 907: ['face', 'grace'], 908: ['heart', 'part'], 909: ['black', 'lack'], 910: ['groan', 'alone'], 911: ['me', 'be'], 912: ['taken', 'forsaken'], 913: ['engrossed', 'crossed'], 914: ['ward', 'guard'], 915: ['bail', 'gaol'], 916: ['thee', 'me'], 917: ['thine', 'mine'], 918: ['will', 'still'], 919: ['free', 'me'], 920: ['kind', 'bind'], 921: ['take', 'sake'], 922: ['use', 'abuse'], 923: ['me', 'free'], 924: ['will', 'still'], 925: ['over-plus', 'thus'], 926: ['spacious', 'gracious'], 927: ['thine', 'shine'], 928: ['still', 'will'], 929: ['store', 'more'], 930: ['kill', \"'will\"], 931: ['near', 'there'], 932: [\"'will\", 'fulfil'], 933: ['love', 'prove'], 934: ['one', 'none'], 935: ['untold', 'hold'], 936: ['be', 'thee'], 937: ['still', 'will'], 938: ['eyes', 'lies'], 939: ['see', 'be'], 940: ['looks', 'hooks'], 941: ['ride', 'tied'], 942: ['plot', 'not'], 943: ['place', 'face'], 944: ['erred', 'transferred'], 945: ['truth', 'youth'], 946: ['lies', 'subtleties'], 947: ['young', 'tongue'], 948: ['best', 'suppressed'], 949: ['unjust', 'trust'], 950: ['old', 'told'], 951: ['me', 'be'], 952: ['wrong', 'tongue'], 953: ['heart', 'art'], 954: ['sight', 'might'], 955: ['aside', 'bide'], 956: ['knows', 'foes'], 957: ['enemies', 'injuries'], 958: ['slain', 'pain'], 959: ['press', 'express'], 960: ['disdain', 'pain'], 961: ['were', 'near'], 962: ['so', 'know'], 963: ['mad', 'bad'], 964: ['thee', 'be'], 965: ['belied', 'wide'], 966: ['eyes', 'despise'], 967: ['note', 'dote'], 968: ['delighted', 'invited'], 969: ['prone', 'alone'], 970: ['can', 'man'], 971: ['thee', 'be'], 972: ['gain', 'pain'], 973: ['hate', 'state'], 974: ['loving', 'reproving'], 975: ['thine', 'mine'], 976: ['ornaments', 'rents'], 977: ['those', 'grows'], 978: ['thee', 'be'], 979: ['hide', 'denied'], 980: ['catch', 'dispatch'], 981: ['away', 'stay'], 982: ['chase', 'face'], 983: ['bent', 'discontent'], 984: ['thee', 'me'], 985: ['behind', 'kind'], 986: ['will', 'still'], 987: ['despair', 'fair'], 988: ['still', 'ill'], 989: ['evil', 'devil'], 990: ['side', 'pride'], 991: ['fiend', 'friend'], 992: ['tell', 'hell'], 993: ['doubt', 'out'], 994: ['make', 'sake'], 995: ['hate', 'state'], 996: ['come', 'doom'], 997: ['sweet', 'greet'], 998: ['end', 'fiend'], 999: ['day', 'away'], 1000: ['threw', 'you'], 1001: ['earth', 'dearth'], 1002: ['array', 'gay'], 1003: ['lease', 'excess'], 1004: ['spend', 'end'], 1005: ['loss', 'dross'], 1006: ['store', 'more'], 1007: ['men', 'then'], 1008: ['still', 'ill'], 1009: ['disease', 'please'], 1010: ['love', 'approve'], 1011: ['kept', 'except'], 1012: ['care', 'are'], 1013: ['unrest', 'expressed'], 1014: ['bright', 'night'], 1015: ['head', 'fled'], 1016: ['sight', 'aright'], 1017: ['dote', 'denote'], 1018: ['so', 'no'], 1019: ['true', 'view'], 1020: ['tears', 'clears'], 1021: ['blind', 'find'], 1022: ['not', 'forgot'], 1023: ['partake', 'sake'], 1024: ['friend', 'spend'], 1025: ['upon', 'moan'], 1026: ['respect', 'defect'], 1027: ['despise', 'eyes'], 1028: ['mind', 'blind'], 1029: ['might', 'sight'], 1030: ['sway', 'day'], 1031: ['ill', 'skill'], 1032: ['deeds', 'exceeds'], 1033: ['more', 'abhor'], 1034: ['hate', 'state'], 1035: ['me', 'thee'], 1036: ['is', 'amiss'], 1037: ['love', 'prove'], 1038: ['betray', 'may'], 1039: ['treason', 'reason'], 1040: ['thee', 'be'], 1041: ['pride', 'side'], 1042: ['call', 'fall'], 1043: ['forsworn', 'torn'], 1044: ['swearing', 'bearing'], 1045: ['thee', 'thee'], 1046: ['most', 'lost'], 1047: ['kindness', 'blindness'], 1048: ['constancy', 'see'], 1049: ['i', 'be'], 1050: ['asleep', 'steep'], 1051: ['found', 'ground'], 1052: ['love', 'prove'], 1053: ['endure', 'cure'], 1054: ['new-fired', 'desired'], 1055: ['breast', 'guest'], 1056: ['lies', 'eyes'], 1057: ['asleep', 'keep'], 1058: ['brand', 'hand'], 1059: ['fire', 'desire'], 1060: ['warmed', 'disarmed'], 1061: ['by', 'remedy'], 1062: ['perpetual', 'thrall'], 1063: ['prove', 'love']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbSsLxQncG-X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_backwards_emission(hmm, start, M):\n",
        "    '''\n",
        "    Generates a backwards emission of length M, assuming that the starting\n",
        "    state is given. \n",
        "\n",
        "    Arguments:\n",
        "        hmm:        Given HMM.\n",
        "        start:      Rhyme end.\n",
        "        M:          Length of the emission to generate.\n",
        "\n",
        "    Returns:\n",
        "        emission:   The randomly generated emission as a list.\n",
        "\n",
        "        states:     The randomly generated states as a list.\n",
        "    '''\n",
        "    # Initialize\n",
        "    emission = [start]\n",
        "    states = [np.random.choice(range(hmm.L), p = normalize(hmm.O[:,start]))]\n",
        "    \n",
        "    for i in range(M - 1):\n",
        "        # Pick the next state depending on the probability\n",
        "        choice = np.random.choice(range(hmm.L), p = normalize(hmm.A[states[i], :]))\n",
        "        states.append(choice)\n",
        "        emission.append(np.random.choice(range(hmm.D), p = normalize(hmm.O[states[i + 1], :])))\n",
        "\n",
        "    return emission, states"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAo-7XnkXm_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rhyme_sequence_generator(hmm, k, M, syllable_dict=syll_dict, int_dict=int_word, word_dict=word_int):\n",
        "    '''\n",
        "    Generates k emissions of length M using the HMM for a given n\n",
        "    and prints the results.\n",
        "\n",
        "    Arguments:\n",
        "        hmm:        HMM to use.\n",
        "        K:          Number of sequences to generate.\n",
        "        M:          Length of emission to generate.\n",
        "        syllable_dict:  Syllable dictionary to use.\n",
        "        int_dict:       Integer to word dictionary to use.\n",
        "        word_dict:      Word to integer dictionary to use.\n",
        "    '''\n",
        "    # Generate k input sequences.\n",
        "    a_rhyme = []\n",
        "    b_rhyme = []\n",
        "    last = False\n",
        "    for i in range(int(k/2)):\n",
        "      choice = np.copy(rhyme_dict[np.random.choice(len(rhyme_dict))])\n",
        "      random.shuffle(choice)\n",
        "      a_rhyme.extend(choice)\n",
        "      choice = np.copy(rhyme_dict[np.random.choice(len(rhyme_dict))])\n",
        "      random.shuffle(choice)\n",
        "      b_rhyme.extend(choice)\n",
        "\n",
        "    for i in range(k):\n",
        "      syllables = 0\n",
        "      if i == k - 2:\n",
        "        word = a_rhyme.pop(0)\n",
        "        last = True\n",
        "      elif last:\n",
        "        word = a_rhyme.pop(0)\n",
        "        last = False\n",
        "      else:\n",
        "        if i % 2 == 0:\n",
        "          word = a_rhyme.pop(0)\n",
        "        else:\n",
        "          word = b_rhyme.pop(0)\n",
        "\n",
        "      while syllables != 10:\n",
        "        # Generate a single input sequence of length m.\n",
        "        emission, states = generate_backwards_emission(hmm, int(word_dict[word]), M)\n",
        "        syllables = 0\n",
        "        generated = \"\"\n",
        "        for i in emission:\n",
        "          if syllables != 10:\n",
        "            if str(int_dict[i]) not in syllable_dict:\n",
        "              syllables = 100\n",
        "              break\n",
        "            syllables += syllable_dict[str(int_dict[i])]\n",
        "            generated = str(int_dict[i]) + \" \" + generated\n",
        "\n",
        "      # Print the results.\n",
        "      print(generated.capitalize())\n",
        "\n",
        "    print('')\n",
        "    print('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJsl92dxaRl_",
        "colab_type": "code",
        "outputId": "0699302d-03e5-4104-854c-549aecd51cef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "# Generate sonnet with Shakespeare's work\n",
        "rhyme_sequence_generator(unsup_HMM, 14, 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "And buds rude shall the disdaineth perceived \n",
            "The to be our that his do derive \n",
            "Sinful he torture heir his deceived \n",
            "Sins out thou then should shows spur can and thrive \n",
            "Your though was thy thou that feeding away \n",
            "Though concealed rearward to with and disdain \n",
            "Parts now all am mock to a mine decay \n",
            "Love with fingers glass that lodged plants much pain \n",
            "Check so decay winged your and with skill \n",
            "Find burthen but for will my on thee me \n",
            "Then were i quickly give they more down ill \n",
            "My everywhere he so of past was be \n",
            "Powerful of skill ordering should i praise \n",
            "In my heart dear of soon darling be days \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJmyDXqVZZeH",
        "colab_type": "text"
      },
      "source": [
        "## Visualization (using the TA's visualization from HW 6)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpKaZcRRZewT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from IPython.display import HTML\n",
        "\n",
        "from HMM_helper import (\n",
        "    text_to_wordcloud,\n",
        "    states_to_wordclouds,\n",
        "    parse_observations,\n",
        "    sample_sentence,\n",
        "    visualize_sparsities,\n",
        "    animate_emission\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jpr2CW7mZq6u",
        "colab_type": "text"
      },
      "source": [
        "Visualize dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXxU534yZoCn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = open('shakespeare.txt').read()\n",
        "wordcloud = text_to_wordcloud(text, title='Shakespeare')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1egSL9XQZuGa",
        "colab_type": "text"
      },
      "source": [
        "Visual sparsity of A and O matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1J5xcmwwZpgT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visualize_sparsities(unsup_HMM, O_max_cols=unsup_HMM.D)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_Z1iuZyZ0yu",
        "colab_type": "text"
      },
      "source": [
        "Visualize word cloud of each state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOwSLkTuZ1IB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wordclouds = states_to_wordclouds(unsup_HMM, word_int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AKVUmYUZ53J",
        "colab_type": "text"
      },
      "source": [
        "Animate generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4VHrCN0Z6Ij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "anim = animate_emission(unsup_HMM, word_int, M=8)\n",
        "HTML(anim.to_html5_video())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}